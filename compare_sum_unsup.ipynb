{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Optional\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "with open('./data/original_data/X_train.json', 'r') as file:\n",
    "    X_train = json.load(file)\n",
    "    \n",
    "with open('./data/original_data/y_train.json', 'r') as file:\n",
    "    y_train = json.load(file)\n",
    "    \n",
    "with open('./data/original_data/X_test.json', 'r') as file:\n",
    "    X_test = json.load(file)\n",
    "    \n",
    "with open('./data/original_data/y_test.json', 'r') as file:\n",
    "    y_test = json.load(file)\n",
    "    \n",
    "with open('./data/synsets/lesk_test_s_synsets.json', 'r') as file:\n",
    "    test_synsets = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions of the ipynb files:\n",
    "def get_sentiment(synset:'Synset') -> Optional[tuple]:\n",
    "    \"\"\"\n",
    "    Get sentiment scores for a synset.\n",
    "\n",
    "    Parameters:\n",
    "        synset (Synset): Synset to analyze.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple with positive, negative and objective scores if synset is found. None otherwise.\n",
    "    \"\"\"\n",
    "    sentiment = swn.senti_synset(synset)\n",
    "    return (sentiment.pos_score(), sentiment.neg_score(), sentiment.obj_score()) if sentiment else None\n",
    "\n",
    "def score_synsets(synsets:list, score:str = 'obj', score_threshold:float = 0, merge_scores:str = 'mean', default:float = 0, hide_warnings:bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Compute a score for each synset in a list of synsets and merge them into a single score.\n",
    "\n",
    "    Parameters:\n",
    "        synsets (list): List of synsets.\n",
    "        score (str): Score to compute. One of 'pos', 'neg', 'obj', 'max_score', 'dif', 'dif2', 'dif_threshold', 'dif2_threshold'.\n",
    "        score_threshold (float): Threshold for 'dif_threshold' and 'dif2_threshold' scores. Scores below this threshold are set to 0.\n",
    "        merge_scores (str): Method for merging scores into a single score. One of 'sum', 'mean', 'max', 'min', 'scale_norm1_mean', 'scale_norm2_mean'.\n",
    "        default (float): Default score to return if synsets is empty.\n",
    "        hide_warnings (bool): Whether to hide warnings or not.\n",
    "        \n",
    "    Returns:\n",
    "        float: Merged score.\n",
    "    \"\"\"\n",
    "    if len(synsets) == 0:\n",
    "        if not hide_warnings:\n",
    "            warnings.warn(f\"Empty synsets list. Returning default score ({default}).\", SyntaxWarning)\n",
    "        return default\n",
    "\n",
    "    if score == 'max_score' and merge_scores not in ['sum', 'mean']:\n",
    "        if not hide_warnings:\n",
    "            warnings.warn(f\"Score 'max_score' is not compatible with '{merge_scores}'. Using 'sum' instead.\", SyntaxWarning)\n",
    "        merge_scores = 'sum'\n",
    "\n",
    "    dict_score = {\n",
    "        'pos': lambda s: s[0],\n",
    "        'neg': lambda s: s[1], \n",
    "        'obj': lambda s: s[2],\n",
    "        'max_score': lambda s: (-1 if s[0] > s[1] else 1) if s[0] != s[1] else 0,\n",
    "        'dif': lambda s: s[0] - s[1],\n",
    "        'dif2': lambda s: s[0]**2 - s[1]**2,\n",
    "        'dif_threshold': lambda s: (s[0] if abs(s[0]) >= score_threshold else 0) - (s[1] if abs(s[1]) >= score_threshold else 0),\n",
    "        'dif2_threshold': lambda s: (s[0]**2 if abs(s[0]) >= score_threshold else 0) - (s[1]**2 if abs(s[1]) >= score_threshold else 0),\n",
    "        }\n",
    "    \n",
    "    assert score in dict_score.keys(), f\"Score '{score}' not valid. Choose one of {list(dict_score.keys())}\"\n",
    "    \n",
    "    def min_max_scale(scores:list[float|int]) -> list[float]:\n",
    "        \"\"\"\n",
    "        Compute min-max scaling of a list of scores.\n",
    "\n",
    "        Parameters:\n",
    "            scores (list): List of scores.\n",
    "\n",
    "        Returns:\n",
    "            list: Scaled scores.\n",
    "        \"\"\"\n",
    "        min_score = min(scores)\n",
    "        max_score = max(scores)\n",
    "        return [(s - min_score) / (max_score - min_score) for s in scores]\n",
    "\n",
    "    dict_merge = {\n",
    "        'sum': lambda sc: sum(sc),\n",
    "        'mean': lambda sc: np.mean(sc),\n",
    "        'max': lambda sc: max(sc),\n",
    "        'min': lambda sc: min(sc),\n",
    "        'scale_norm1_mean': lambda sc: np.mean(np.abs(min_max_scale(sc))),\n",
    "        'scale_norm2_mean': lambda sc: np.linalg.norm(min_max_scale(sc)) / len(sc),\n",
    "    }\n",
    "    \n",
    "    assert merge_scores in dict_merge.keys(), f\"Merge score '{merge_scores}' not valid. Choose one of {list(dict_merge.keys())}\"\n",
    "\n",
    "    score_func = dict_score[score]\n",
    "    scores = [score_func(get_sentiment(synset=s)) for s in synsets if s is not None]\n",
    "\n",
    "    if merge_scores in ['scale_norm1_mean', 'scale_norm2_mean'] and min(scores) == max(scores):\n",
    "        if not hide_warnings:\n",
    "            warnings.warn(f\"Scores are all the same and cannot be scaled. Returning default score ({default}).\", RuntimeWarning)\n",
    "        return default\n",
    "\n",
    "\n",
    "    merge_func = dict_merge[merge_scores]\n",
    "    scores_merged = merge_func(scores)\n",
    "\n",
    "    return scores_merged\n",
    "\n",
    "def discretize_scores(scores:list, threshold:float, positive_value = 1, negative_value = 0) -> list:\n",
    "    \"\"\"\n",
    "    Returns a list of binary values based on a threshold.\n",
    "\n",
    "    Parameters:\n",
    "        scores (list): List of scores.\n",
    "        threshold (float): Minimum value to consider a score as positive.\n",
    "        positive_value: Value to assign to positive scores.\n",
    "        negative_value: Value to assign to negative scores.\n",
    "    \"\"\"\n",
    "    return list(map(lambda x: positive_value if x >= threshold else negative_value, scores))\n",
    "\n",
    "\n",
    "def run_experiment(synsets:list, y:list, allowed:list, score:str, merge:str, thresh:float, score_thresh:float = 0, hide_warnings:bool = False, continuous:bool = False) -> tuple[float, list]:\n",
    "    \"\"\"\n",
    "    Run an experiment with a set of parameters.\n",
    "\n",
    "    Parameters:\n",
    "        synsets (list): List of synsets.\n",
    "        y (list): List of labels.\n",
    "        allowed (list): Allowed POS tags.\n",
    "        score (str): Score to compute.\n",
    "        merge (str): Method to merge scores.\n",
    "        thresh (float): Threshold for discretization.\n",
    "        score_thresh (float): Threshold for 'dif_threshold' and 'dif2_threshold' scores.\n",
    "        hide_warnings (bool): Whether to hide warnings or not.\n",
    "        continuous (bool): Wheter to return continuous scores or not.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Accuracy (float) and results (list of predictions).\n",
    "    \"\"\"\n",
    "    scores_opinions = []\n",
    "    for opinion in synsets:\n",
    "        scores_sentences = []\n",
    "        for sentence in opinion:\n",
    "            filter_sentence = [name for name in sentence if name.split('.')[1] in allowed]\n",
    "            scores_sentences.append(score_synsets(synsets=filter_sentence, score=score, merge_scores=merge, score_threshold=score_thresh, hide_warnings=hide_warnings))\n",
    "\n",
    "        scores_opinions.append(np.mean(scores_sentences))\n",
    "\n",
    "    if not continuous:\n",
    "        results_opinions = discretize_scores(scores=scores_opinions, threshold=thresh)\n",
    "\n",
    "        accuracy = accuracy_score(y, results_opinions)\n",
    "    else:\n",
    "        results_opinions = scores_opinions\n",
    "        accuracy = 0.0\n",
    "\n",
    "    return accuracy, results_opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_wrong_predictions(y_true, y_pred1, y_pred2):\n",
    "\t\"\"\"\n",
    "\tReturns accuracy of model 2 in the wrong predictions of model 1.\n",
    "\n",
    "\tParameters:\n",
    "\t\ty_true (list): List of true labels.\n",
    "\t\ty_pred1 (list): List of predictions of model 1.\n",
    "\t\ty_pred2 (list): List of predictions of model 2.\n",
    "\t\n",
    "\tReturns:\n",
    "\t\tfloat: Accuracy of model 2 in the wrong predictions of model 1.\n",
    "\t\"\"\"\n",
    "\n",
    "\twrong_indices = [i for i in range(len(y_true)) if y_true[i] != y_pred1[i]]\n",
    "\ty_true_wrong = [y_true[i] for i in wrong_indices]\n",
    "\ty_pred2_wrong = [y_pred2[i] for i in wrong_indices]\n",
    "\n",
    "\treturn accuracy_score(y_true_wrong, y_pred2_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unsupervised model in the wrong predictions of supervised model: 0.45\n",
      "Accuracy of supervised model in the wrong predictions of unsupervised model: 0.78\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy with wrong predictions of\n",
    "\n",
    "# Model supervised\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_alternate = CountVectorizer(min_df=0.0, binary=True, stop_words='english', strip_accents='ascii')\n",
    "\n",
    "X_train_alt = vectorizer_alternate.fit_transform(X_train)\n",
    "X_test_alt = vectorizer_alternate.transform(X_test)\n",
    "\n",
    "best_clf = RandomForestClassifier(max_depth=14, n_estimators=1500, random_state=42)\n",
    "best_clf.fit(X_train_alt, y_train)\n",
    "y_pred_sup = best_clf.predict(X_test_alt)\n",
    "\n",
    "# Model unsupervised\n",
    "_, y_pred_unsup = run_experiment(synsets=test_synsets, y=y_test, allowed=['a', 's', 'r', 'n'], score='dif', merge='sum', thresh=0, hide_warnings=True)\n",
    "\n",
    "# Compare wrong predictions\n",
    "acc_unsup = compare_wrong_predictions(y_true=y_test, y_pred1=y_pred_sup, y_pred2=y_pred_unsup)\n",
    "print(f\"Accuracy of unsupervised model in the wrong predictions of supervised model: {acc_unsup:.2f}\")\n",
    "\n",
    "acc_sup = compare_wrong_predictions(y_true=y_test, y_pred1=y_pred_unsup, y_pred2=y_pred_sup)\n",
    "print(f\"Accuracy of supervised model in the wrong predictions of unsupervised model: {acc_sup:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
