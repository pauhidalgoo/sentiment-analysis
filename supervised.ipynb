{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SENTIMENT ANALYSIS - SUPERVISED**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicialització"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialitzem els imports necessaris, així com les lectures dels diferents documents i el preprocessament dels texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "with open('./data/original_data/X_train.json', 'r') as file:\n",
    "    X_train_data = json.load(file)\n",
    "    \n",
    "with open('./data/original_data/X_test.json', 'r') as file:\n",
    "    X_test_data = json.load(file)\n",
    "\n",
    "with open('./data/original_data/X_train_no_val.json', 'r') as file:\n",
    "    X_train_val_data = json.load(file)\n",
    "\n",
    "with open('./data/original_data/X_val.json', 'r') as file:\n",
    "    X_val_data = json.load(file)\n",
    "    \n",
    "with open('./data/original_data//y_train.json', 'r') as file:\n",
    "    y_train = json.load(file)\n",
    "    \n",
    "with open('./data/original_data/y_test.json', 'r') as file:\n",
    "    y_test = json.load(file)\n",
    "\n",
    "with open('./data/original_data/y_train_no_val.json', 'r') as file:\n",
    "    y_train_val = json.load(file)\n",
    "\n",
    "with open('./data/original_data/y_val.json', 'r') as file:\n",
    "    y_val = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=0.0, stop_words='english', strip_accents='ascii')\n",
    "\n",
    "# Vectorizer alternatiu per comparar resultats amb l'altre\n",
    "vectorizer_alternate = CountVectorizer(min_df=0.0, binary=True, stop_words='english', strip_accents='ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEn cas de voler Stemmatitzar:\\nfrom nltk.stem import PorterStemmer\\nstemmer = PorterStemmer()\\nX_train_stem = [\" \".join([stemmer.stem(i) for i in word_tokenize(sent)]) for sent in X_train_data]\\nX_test_stem = [\" \".join([stemmer.stem(i) for i in word_tokenize(sent)]) for sent in X_test_data]\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Realitzem el preprocessament esmentat: lemmatitzar, treure signes de puntuació i stopwords\n",
    "wnl = WordNetLemmatizer()\n",
    "X_train_lem = [\" \".join([wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(word_tokenize(sent))]) for sent in X_train_data]\n",
    "X_test_lem = [\" \".join([wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(word_tokenize(sent))]) for sent in X_test_data]\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "X_train_pre = []\n",
    "for sent in X_train_lem:\n",
    "    sentence = []\n",
    "    for token in word_tokenize(sent):\n",
    "        if token not in string.punctuation and token not in stop_words and token != \"``\":\n",
    "            sentence.append(token)\n",
    "    X_train_pre.append(\" \".join(e for e in sentence))\n",
    "\n",
    "X_test_pre = []\n",
    "for sent in X_test_lem:\n",
    "    sentence = []\n",
    "    for token in word_tokenize(sent):\n",
    "        if token not in string.punctuation and token not in stop_words and token not in [\"``\", \"--\"]:\n",
    "            sentence.append(token)\n",
    "    X_test_pre.append(\" \".join(e for e in sentence))\n",
    "\n",
    "\"\"\"\n",
    "En cas de voler Stemmatitzar:\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "X_train_stem = [\" \".join([stemmer.stem(i) for i in word_tokenize(sent)]) for sent in X_train_data]\n",
    "X_test_stem = [\" \".join([stemmer.stem(i) for i in word_tokenize(sent)]) for sent in X_test_data]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realitzem els fits i els transforms tant per les dades que usarem d'entrada (\"validació\") com per les després usades en train i test. Tenir en compte que aquesta primera validació és tan sols per escollir alguns models que més o menys funcionin, i que la validació real es farà utilitzant Cross-Validation més endavant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train_stem = vectorizer.fit_transform(X_train_stem)\\nX_test_stem = vectorizer.transform(X_test_stem)\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dades sense processar, vectorizer normal, de validació\n",
    "X_train_val = vectorizer.fit_transform(X_train_val_data)\n",
    "X_val = vectorizer.transform(X_val_data)\n",
    "\n",
    "# Dades sense processar, vectorizer normal, de train test\n",
    "X_train = vectorizer.fit_transform(X_train_data)\n",
    "X_test = vectorizer.transform(X_test_data)\n",
    "\n",
    "# Dades sense processar, vectorizer alternatiu, de validació\n",
    "X_train_val_alt = vectorizer_alternate.fit_transform(X_train_val_data)\n",
    "X_val_alt = vectorizer_alternate.transform(X_val_data)\n",
    "\n",
    "# Dades sense processar, vectorizer alternatiu, de train test\n",
    "X_train_alt = vectorizer_alternate.fit_transform(X_train_data)\n",
    "X_test_alt = vectorizer_alternate.transform(X_test_data)\n",
    "\n",
    "# Dades lemmatitzades, vectorizer normal, de train test\n",
    "X_train_lem = vectorizer.fit_transform(X_train_lem)\n",
    "X_test_lem = vectorizer.transform(X_test_lem)\n",
    "\n",
    "# Dades amb el preprocessament, vectorizer normal, de train test\n",
    "X_train_pre = vectorizer.fit_transform(X_train_pre)\n",
    "X_test_pre = vectorizer.transform(X_test_pre)\n",
    "\n",
    "\"\"\"\n",
    "X_train_stem = vectorizer.fit_transform(X_train_stem)\n",
    "X_test_stem = vectorizer.transform(X_test_stem)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7786666666666666"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5, max_depth=3, random_state=0).fit(X_train_val, y_train_val)\n",
    "clf.score(X_val, y_val)\n",
    "\n",
    "# 0.7787"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.808"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\", random_state=0)\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "accuracy_score(clf.predict(X_val), y_val)\n",
    "\n",
    "# 0.808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6906666666666667"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train_val.toarray(), y_train_val)\n",
    "\n",
    "accuracy_score(clf.predict(X_val.toarray()), y_val)\n",
    "\n",
    "# 0.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8693333333333333"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=800, max_depth=12, random_state=0)\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "accuracy_score(clf.predict(X_val), y_val)\n",
    "\n",
    "# 0.8693"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8213333333333334"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "accuracy_score(clf.predict(X_val), y_val)\n",
    "\n",
    "# 0.8213"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7813333333333333"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(C=1, kernel='rbf', degree=3)\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "accuracy_score(clf.predict(X_val), y_val)\n",
    "\n",
    "# 0.7813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5413333333333333"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "accuracy_score(clf.predict(X_val), y_val)\n",
    "\n",
    "# 0.5413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.832"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-3, hidden_layer_sizes=(20, 10), random_state=42)\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "accuracy_score(clf.predict(X_val), y_val)\n",
    "\n",
    "# 0.832"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observem també, com canvia per exemple el Random Forest i el Logistic Regression amb l'altre vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.856"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=800, max_depth=12, random_state=0)\n",
    "clf.fit(X_train_val_alt, y_train_val)\n",
    "\n",
    "accuracy_score(clf.predict(X_val_alt), y_val)\n",
    "\n",
    "# 0.8666\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train_val_alt, y_train_val)\n",
    "\n",
    "accuracy_score(clf.predict(X_val_alt), y_val)\n",
    "\n",
    "# 0.856"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquest apartat, realitzarem una cross-validation per cada model (per escollir els seus millors paràmetres), i després agafarem el millor de tots els models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier: Best params = {'learning_rate': 0.25, 'max_depth': 6, 'random_state': 0} (Accuracy = 0.7947)\n",
      "AdaBoostClassifier: Best params = {'algorithm': 'SAMME', 'learning_rate': 1, 'n_estimators': 200, 'random_state': 0} (Accuracy = 0.7907)\n",
      "RandomForestClassifier: Best params = {'max_depth': 14, 'n_estimators': 1500, 'random_state': 0} (Accuracy = 0.8613)\n",
      "LogisticRegression: Best params = {'max_iter': 1000, 'random_state': 0} (Accuracy = 0.8513)\n",
      "SVC: Best params = {'C': 1, 'degree': 1, 'kernel': 'poly', 'random_state': 0} (Accuracy = 0.8520)\n",
      "MLPClassifier: Best params = {'alpha': 0.001, 'hidden_layer_sizes': (20, 10), 'learning_rate': 'constant', 'random_state': 0, 'solver': 'lbfgs'} (Accuracy = 0.8593)\n",
      "BEST MODEL: RandomForestClassifier with params = {'max_depth': 14, 'n_estimators': 1500, 'random_state': 0} (Accuracy = 0.8613)\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# List of models and their hyperparameters\n",
    "random_state = 0\n",
    "models_params = [\n",
    "    (GradientBoostingClassifier(), {'learning_rate': [0.1, 0.25, 0.5], 'max_depth': [3, 6], 'random_state': [random_state]}),\n",
    "    (AdaBoostClassifier(), {'n_estimators': [50, 100, 200], 'algorithm': [\"SAMME\", \"SAMME.R\"], 'learning_rate': [0.5, 1], 'random_state': [random_state]}),\n",
    "    (RandomForestClassifier(), {'n_estimators': [100, 500, 1000, 1500], 'max_depth': [2, 5, 10, 12, 13, 14, None], 'random_state': [random_state]}),\n",
    "    (LogisticRegression(), {'max_iter': [1000] , 'random_state': [random_state]}),\n",
    "    (SVC(), {'C': [0.5, 1, 5], 'kernel': ['linear', 'rbf', 'poly'], 'degree': [1, 2, 3], 'random_state': [random_state]}),\n",
    "    (MLPClassifier(), {'solver': ['lbfgs', 'adam'], 'learning_rate': ['constant', 'adaptive', 'invscaling'], 'hidden_layer_sizes': [(5, 2), (10, 5), (20, 10)], 'alpha': [1e-2, 1e-3, 1e-5], 'random_state': [random_state]})\n",
    "]\n",
    "results = []\n",
    "\n",
    "def run_grid_search(model, params):\n",
    "    \"\"\"\n",
    "    Runs a grid search for the given model and parameters.\n",
    "    \"\"\"\n",
    "    grid_search = GridSearchCV(model, params, scoring='accuracy', cv=5)\n",
    "    grid_search.fit(X_train_alt, y_train)\n",
    "    \n",
    "    return grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "# Run grid search for each model\n",
    "iter = 1\n",
    "for model, params in models_params:\n",
    "    print(f\"Running grid search on model: {iter}/{len(models_params)}\\r\", end=\"\")\n",
    "    best_params, best_score = run_grid_search(model, params)\n",
    "    results.append((model.__class__.__name__, best_params, best_score))\n",
    "    iter += 1\n",
    "\n",
    "# Show results\n",
    "for model_name, best_params, best_score in results:\n",
    "    print(f\"{model_name}: Best params = {best_params} (Accuracy = {best_score:.4f})\")\n",
    "\n",
    "best_model = max(results, key=lambda x: x[2])\n",
    "print(f\"BEST MODEL: {best_model[0]} with params = {best_model[1]} (Accuracy = {best_model[2]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.854"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the best model on the test set\n",
    "best_clf = RandomForestClassifier(max_depth=14, n_estimators=1500, random_state=42)\n",
    "best_clf.fit(X_train_alt, y_train)\n",
    "accuracy_score(best_clf.predict(X_test_alt), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = best_clf.predict(X_test_alt)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier( random_state=42)\n",
    "clf.fit(X_train_alt, y_train)\n",
    "accuracy_score(clf.predict(X_test_alt), y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
