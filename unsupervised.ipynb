{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SENTIMENT ANALYSIS - UNSUPERVISED**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "from textserver import TextServer\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Optional\n",
    "import warnings\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('login.env')\n",
    "ts_password = os.getenv(\"PASSWORD_PAU\")\n",
    "ts_user = os.getenv(\"USER_PAU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets    \n",
    "with open('./data/X_test.json', 'r') as file:\n",
    "    X_test = json.load(file)\n",
    "    \n",
    "with open('./data/y_test.json', 'r') as file:\n",
    "    y_test = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_text(text):\n",
    "    fixed_text = re.sub(r'[\\\\\"\\+\\/]', '', text)\n",
    "    fixed_text = re.sub(r'\\s*([.,!\\?;:])\\s*', r'\\1 ', fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' s\\s+', \"'s \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' t\\s+', \"'t \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' re\\s+', \"'re \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' ve\\s+', \"'ve \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' ll\\s+', \"'ll \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+', ' ', fixed_text)\n",
    "    fixed_text = re.sub(r'\\si\\s', ' I ', fixed_text)\n",
    "    fixed_text = re.sub(r'(?:^|(?<=[.!?]))\\s*(\\w)', lambda x: x.group(1).upper(), fixed_text)\n",
    "    return fixed_text.strip()\n",
    "\n",
    "def get_sentences(text:str, remove_stopwords:bool = False) -> None:\n",
    "    sent_list = nltk.sent_tokenize(text)\n",
    "    if remove_stopwords:\n",
    "        no_stopwords_sentences = []\n",
    "        for sentence in sent_list:\n",
    "            word_tokens = word_tokenize(sentence)\n",
    "            filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "            filtered_sentence = []\n",
    "            for w in word_tokens:\n",
    "                if w not in stop_words:\n",
    "                    filtered_sentence.append(w)\n",
    "            no_stopwords_sentences.append(filtered_sentence)\n",
    "        return no_stopwords_sentences\n",
    "    else:\n",
    "        return sent_list\n",
    "    \n",
    "\n",
    "def get_sentiment(synset:'Synset'):\n",
    "    sentiment = swn.senti_synset(synset)\n",
    "    return (sentiment.pos_score(), sentiment.neg_score(), sentiment.obj_score()) if sentiment else None\n",
    "\n",
    "def score_synsets(synsets:list, score:str = 'obj', threshold:float = 0.25, merge_scores:str = 'mean') -> float:\n",
    "    \"\"\"\n",
    "    Compute a score for each synset in a list of synsets and merge them into a single score.\n",
    "\n",
    "    Parameters:\n",
    "        synsets (list): List of synsets.\n",
    "        score (str): Score to compute. One of 'pos', 'neg', 'obj', 'max_score', 'dif', 'dif2', 'dif_threshold', 'dif2_threshold', 'dif_obj', 'dif2_obj'.\n",
    "        threshold (float): Threshold for 'dif_threshold' and 'dif2_threshold' scores.\n",
    "        merge_scores (str): Method for merging scores into a single score. One of 'sum', 'mean', 'max', 'min', 'scale', 'scale_norm1_mean', 'scale_norm2_mean'.\n",
    "\n",
    "    Returns:\n",
    "        float: Merged score.\n",
    "    \"\"\"\n",
    "\n",
    "    if score == 'max_score' and merge_scores not in ['sum', 'mean']:\n",
    "        warnings.warn(f\"Score 'max_score' is not compatible with '{merge_scores}'. Using 'sum' instead.\", SyntaxWarning)\n",
    "        merge_scores = 'sum'\n",
    "\n",
    "\n",
    "    dict_score = {\n",
    "        'pos': lambda s: s[0],\n",
    "        'neg': lambda s: s[1], \n",
    "        'obj': lambda s: s[2],\n",
    "        'max_score': lambda s: (-1 if s[0] > s[1] else 1) if s[0] != s[1] else 0,\n",
    "        'dif': lambda s: s[0] - s[1],\n",
    "        'dif2': lambda s: s[0]**2 - s[1]**2,\n",
    "        'dif_threshold': lambda s: (s[0] if s[0] > threshold else 0) - (s[1] if s[1] > threshold else 0),\n",
    "        'dif2_threshold': lambda s: (s[0]**2 if s[0] > threshold else 0) - (s[1]**2 if s[1] > threshold else 0),\n",
    "        'dif_obj': lambda s: (s[0] - s[1]) * s[2],\n",
    "        'dif2_obj': lambda s: (s[0]**2 - s[1]**2) * s[2]\n",
    "        }\n",
    "    \n",
    "    assert score in dict_score.keys(), f\"Score '{score}' not valid. Choose one of {list(dict_score.keys())}\"\n",
    "    \n",
    "    def min_max_scale(scores:list) -> list:\n",
    "        min_score = min(scores)\n",
    "        max_score = max(scores)\n",
    "        return [(s - min_score) / (max_score - min_score) for s in scores]\n",
    "\n",
    "    dict_merge = {\n",
    "        'sum': lambda sc: sum(sc),\n",
    "        'mean': lambda sc: np.mean(sc),\n",
    "        'max': lambda sc: max(sc),\n",
    "        'min': lambda sc: min(sc),\n",
    "        'scale_norm1_mean': lambda sc: np.mean(np.abs(min_max_scale(sc))),\n",
    "        'scale_norm2_mean': lambda sc: np.linalg.norm(min_max_scale(sc)) / len(sc),\n",
    "    }\n",
    "    \n",
    "    assert merge_scores in dict_merge.keys(), f\"Merge score '{merge_scores}' not valid. Choose one of {list(dict_merge.keys())}\"\n",
    "\n",
    "    score_func = dict_score[score]\n",
    "    scores = [score_func(get_sentiment(synset=s)) for s in synsets if s is not None]\n",
    "\n",
    "    merge_func = dict_merge[merge_scores]\n",
    "    scores_merged = merge_func(scores)\n",
    "\n",
    "    return scores_merged\n",
    "\n",
    "def discretize_scores(scores:list, threshold:float, positive_value = 1, negative_value = 0) -> list:\n",
    "    \"\"\"\n",
    "    Returns a list of binary values based on a threshold.\n",
    "\n",
    "    Parameters:\n",
    "        scores (list): List of scores.\n",
    "        threshold (float): Minimum value to consider a score as positive.\n",
    "        positive_value: Value to assign to positive scores.\n",
    "        negative_value: Value to assign to negative scores.\n",
    "    \"\"\"\n",
    "    return list(map(lambda x: positive_value if x >= threshold else negative_value, scores))\n",
    "\n",
    "\n",
    "def negativize(sentence):\n",
    "    \"\"\"\n",
    "    Based on https://gist.github.com/UtkarshRedd/3fbfd354ea7a6f83bd8f9419a27b0543\n",
    "    \"\"\"\n",
    "    for i in range(2, len(sentence)):\n",
    "        for distance in [1,2]:\n",
    "            if sentence[i-distance] in ['not', \"n't\", \"t\", \"DELETED\"]:\n",
    "                antonyms = []\n",
    "                for syn in wn.synsets(sentence[i]):\n",
    "                    for l in syn.lemmas():\n",
    "                        if l.antonyms():\n",
    "                            antonyms.append(l.antonyms()[0].name())\n",
    "                max_dissimilarity = 0\n",
    "                antonym_max = None\n",
    "                for ant in antonyms:\n",
    "                    for syn in wn.synsets(ant):\n",
    "                        w1 = wn.synsets(sentence[i])[0].name()\n",
    "                        w2 = syn.name()\n",
    "                        word1 = wn.synset(w1)\n",
    "                        word2 = wn.synset(w2)\n",
    "                        if isinstance(word1.path_similarity(word2), (float, int)):\n",
    "                            temp = 1 - word1.path_similarity(word2)\n",
    "                            if temp > max_dissimilarity:\n",
    "                                max_dissimilarity = temp\n",
    "                                antonym_max = ant\n",
    "                if antonym_max:\n",
    "                    sentence[i] = antonym_max\n",
    "                    sentence[i-distance] = 'DELETED'\n",
    "    sentence = [word for word in sentence if word != 'DELETED']\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LESK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lesk_synsets(text:str, lemmatize:bool = False, remove_stopwords:bool = True, negatives:bool = False):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = []\n",
    "    if lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    if remove_stopwords:\n",
    "        tokens = [w for w in tokens if not w.lower() in stop_words]\n",
    "    if negatives:\n",
    "        tokens = negativize(tokens)\n",
    "    if lemmatize or remove_stopwords or negatives:\n",
    "        tagged_tokens = [a for a in nltk.pos_tag(tokens, tagset=\"universal\")]\n",
    "    else:\n",
    "        tagged_tokens = [(a.text, a.pos_) for a in nlp(text)]\n",
    "    words = []\n",
    "    for token, pos in tagged_tokens:\n",
    "        if pos == \"NOUN\":\n",
    "            syn = lesk(tokens, token, pos=\"n\")\n",
    "        elif pos == \"ADJ\":\n",
    "            syn = lesk(tokens, token, pos=\"a\")\n",
    "        elif pos == \"ADV\":\n",
    "            syn = lesk(tokens, token, pos=\"r\")\n",
    "        elif pos == \"VERB\":\n",
    "            syn = lesk(tokens, token, pos=\"v\")\n",
    "        else:\n",
    "            syn = None\n",
    "        if syn is not None:\n",
    "            words.append(syn)\n",
    "    return words\n",
    "\n",
    "def get_lesk_all_synsets(sentences:list, lemmatizer:bool = True) -> list:\n",
    "    all = []\n",
    "    for sentence in sentences:\n",
    "        all.append(get_lesk_synsets(sentence, lemmatizer))\n",
    "    return all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n"
     ]
    }
   ],
   "source": [
    "test_synsets = []\n",
    "i = 0\n",
    "for opinion in X_test:\n",
    "    s = get_sentences(opinion)\n",
    "    #s = [fix_text(t) for t in s]\n",
    "    syns = get_lesk_all_synsets(s, lemmatizer=False)\n",
    "    names = [[syn.name() for syn in ll] for ll in syns]\n",
    "    test_synsets.append(names)\n",
    "    print(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/lesk_test_stop_synsets.json', 'w') as file:\n",
    "    json.dump(test_synsets, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UKB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Own implementation of UKB since TextServer didn't allow us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ukb import *\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "try:\n",
    "    ukb_graph = load_ukb_graph(\"ukb_graph.gexf\")\n",
    "except:\n",
    "    print(\"Creating graph...\")\n",
    "    ukb_graph = build_ukb_graph()\n",
    "    nx.write_gexf(ukb_graph, \"ukb_graph.gexf\")\n",
    "\n",
    "ukb = UKB(ukb_graph)\n",
    "def get_ukb_synsets(text:str):\n",
    "    context_words = extract_context_words(text)\n",
    "    disambiguated_senses = ukb.disambiguate_context(context_words, method=1)\n",
    "    return list(disambiguated_senses.values())\n",
    "\n",
    "def get_ukb_all_synsets(sentences:list) -> list:\n",
    "    all = []\n",
    "    for sentence in sentences:\n",
    "        all.append([a for a in get_ukb_synsets(sentence) if a != None])\n",
    "    return all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'find': 'find.v.01', 'solution': 'solution.n.01', 'problem': 'problem.n.01', 'electro': None, 'latino': 'latin_american.n.01', 'house': 'house.n.06', 'rock': 'rock.n.02', 'music': 'music.n.01', 'hell': 'hell.n.03', 'sky': 'sky.n.01'}\n"
     ]
    }
   ],
   "source": [
    "context_words = extract_context_words(\"find the solution to this problem electro latino house rock music what hell sky\")\n",
    "disambiguated_senses = ukb.disambiguate_context(context_words, method=1)\n",
    "print(disambiguated_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232\r"
     ]
    }
   ],
   "source": [
    "test_synsets = []\n",
    "for i, opinion in enumerate(X_test):\n",
    "    s = get_sentences(opinion)\n",
    "    #s = [fix_text(t) for t in s]\n",
    "    syns = get_ukb_all_synsets(s)\n",
    "    test_synsets.append(syns)\n",
    "    print(i, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ukb2_test_synsets2.json', 'w') as file:\n",
    "    json.dump(test_synsets, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using just the most freqüent synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = json.load(open(\"./data/word_sense_frequencies_semcor.json\"))\n",
    "\n",
    "def get_freq_synsets(text:str):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = [(a.text, a.pos_) for a in nlp(text)]\n",
    "    words = []\n",
    "    for token, pos in tagged_tokens:\n",
    "        if token not in frequencies.keys():\n",
    "            syn = None\n",
    "        else:\n",
    "            if pos == \"NOUN\":\n",
    "                syn = max(frequencies[token], key=lambda key: frequencies[token][key])\n",
    "            elif pos == \"ADJ\":\n",
    "                syn = max(frequencies[token], key=lambda key: frequencies[token][key])\n",
    "            elif pos == \"ADV\":\n",
    "                syn = max(frequencies[token], key=lambda key: frequencies[token][key])\n",
    "            elif pos == \"VERB\":\n",
    "                syn = max(frequencies[token], key=lambda key: frequencies[token][key])\n",
    "            else:\n",
    "                syn = None\n",
    "        if syn is not None:\n",
    "            words.append(syn.name() if syn.__class__.__name__ == \"Lemma\" else syn)\n",
    "    return words\n",
    "\n",
    "def get_freq_all_synsets(sentences:list) -> list:\n",
    "    all = []\n",
    "    for sentence in sentences:\n",
    "        all.append([a for a in get_freq_synsets(sentence) if a != None])\n",
    "    return all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\r"
     ]
    }
   ],
   "source": [
    "test_synsets = []\n",
    "for i, opinion in enumerate(X_test):\n",
    "    s = get_sentences(opinion)\n",
    "    #s = [fix_text(t) for t in s]\n",
    "    syns = get_freq_all_synsets(s)\n",
    "    test_synsets.append(syns)\n",
    "    print(i, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/freq_test_synsets.json', 'w') as file:\n",
    "    json.dump(test_synsets, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ukb1_test_synsets.json', 'r') as file:\n",
    "\ttest_synsets = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed = [  \"v\", \"a\", \"s\", \"n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gifted.s.00\n",
      "non.s.00\n",
      "condensed.s.00\n",
      "railway.n.1;2\n",
      "anti.s.00\n",
      "post.s.00\n",
      "anti.s.00\n",
      "growing.s.00\n",
      "talented.s.00\n",
      "ish.s.00\n",
      "lingering.s.00\n",
      "hearted.a.00\n",
      "semi.s.00\n",
      "post.s.00\n",
      "post.s.00\n",
      "gifted.s.00\n",
      "post.s.00\n",
      "bloated.s.00\n",
      "touring.s.00\n",
      "growing.s.00\n",
      "anti.s.00\n",
      "anti.s.00\n",
      "spark.n.2;1\n",
      "growing.s.00\n",
      "talented.s.00\n",
      "finest.s.00\n",
      "biggest.s.00\n",
      "talented.s.00\n",
      "bloated.s.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wares.n.00\n",
      "growing.s.00\n",
      "non.s.00\n",
      "talented.s.00\n",
      "guerrilla.s.00\n",
      "depressing.s.00\n",
      "post.s.00\n",
      "anti.s.00\n",
      "tuned.s.00\n",
      "hearted.a.00\n",
      "slightest.s.00\n",
      "spark.n.2;1\n",
      "multi.s.00\n",
      "growing.s.00\n",
      "furnishings.n.00\n",
      "growing.s.00\n",
      "anti.s.00\n",
      "growing.s.00\n",
      "tuned.s.00\n",
      "growing.s.00\n",
      "growing.s.00\n",
      "depressing.s.00\n",
      "sweaty.s.00\n",
      "anti.s.00\n",
      "growing.s.00\n",
      "pre.s.00\n",
      "spark.n.2;1\n",
      "infested.s.00\n",
      "semi.s.00\n",
      "talented.s.00\n",
      "talented.s.00\n",
      "talented.s.00\n",
      "biggest.s.00\n",
      "haired.s.00\n",
      "compassionate.s.00\n",
      "sway.v.0;1\n",
      "unbearable.s.00\n",
      "non.s.00\n",
      "depressing.s.00\n",
      "goods.n.00\n",
      "non.s.00\n",
      "anti.s.00\n",
      "scrap.s.00\n",
      "rotting.s.00\n",
      "smart.s.0;2\n",
      "consist_of.v.00\n",
      "talented.s.00\n",
      "whining.s.00\n",
      "rival.s.00\n",
      "millions.n.00\n",
      "post.s.00\n",
      "semi.s.00\n",
      "calming.s.00\n",
      "growing.s.00\n",
      "sandy.s.00\n",
      "depressing.s.00\n",
      "lingering.s.00\n",
      "lower.s.00\n",
      "post.s.00\n",
      "birthe.v.00\n",
      "lower.s.00\n",
      "rival.s.00\n",
      "spark.n.2;1\n",
      "pre.s.00\n",
      "spark.n.2;1\n",
      "depressing.s.00\n",
      "biggest.s.00\n",
      "multi.s.00\n",
      "semi.s.00\n",
      "non.s.00\n",
      "slightest.s.00\n",
      "growing.s.00\n",
      "consist_of.v.00\n",
      "anti.s.00\n",
      "consist_of.v.00\n",
      "spoil.v.3;1\n",
      "pilot.s.00\n",
      "carping.s.00\n",
      "sunshiny.s.00\n",
      "reserve.s.00\n",
      "post.s.00\n",
      "docked.a.00\n",
      "consist_of.v.00\n",
      "depressing.s.00\n",
      "consist_of.v.00\n",
      "fashions.n.00\n",
      "rival.s.00\n",
      "post.s.00\n",
      "lower.s.00\n",
      "lower.s.00\n",
      "growing.s.00\n",
      "supplies.n.00\n",
      "biggest.s.00\n",
      "hills.n.00\n",
      "non.s.00\n",
      "non.s.00\n",
      "anti.s.00\n",
      "post.s.00\n",
      "sweaty.s.00\n",
      "depressing.s.00\n",
      "pilot.s.00\n",
      "pilot.s.00\n",
      "gloves.n.00\n",
      "pilot.s.00\n",
      "pilot.s.00\n",
      "pre.s.00\n",
      "pilot.s.00\n",
      "unbearable.s.00\n",
      "non.s.00\n",
      "semi.s.00\n",
      "depressing.s.00\n",
      "post.s.00\n",
      "biggest.s.00\n",
      "hills.n.00\n",
      "hills.n.00\n",
      "hills.n.00\n",
      "pilot.s.00\n",
      "pilot.s.00\n",
      "pilot.s.00\n",
      "pilot.s.00\n",
      "pilot.s.00\n",
      "multi.s.00\n",
      "non.s.00\n",
      "semi.s.00\n",
      "talented.s.00\n",
      "biggest.s.00\n",
      "powered.s.00\n",
      "talented.s.00\n",
      "talented.s.00\n",
      "biggest.s.00\n",
      "talented.s.00\n",
      "rival.s.00\n",
      "egocentric.s.00\n",
      "talented.s.00\n",
      "multi.s.00\n",
      "biggest.s.00\n",
      "whining.s.00\n",
      "ish.s.00\n",
      "showy.s.0;1\n",
      "non.s.00\n",
      "festering.s.00\n",
      "depressing.s.00\n",
      "biggest.s.00\n",
      "anti.s.00\n",
      "highest.s.00\n",
      "finest.s.00\n",
      "bloated.s.00\n",
      "pilot.s.00\n",
      "consist_of.v.00\n",
      "harmful.s.00\n",
      "multi.s.00\n",
      "talented.s.00\n",
      "goods.n.00\n",
      "oldest.s.00\n",
      "pre.s.00\n",
      "sport.n.2;1\n",
      "post.s.00\n",
      "post.s.00\n",
      "growing.s.00\n",
      "growing.s.00\n",
      "biggest.s.00\n",
      "consist_of.v.00\n",
      "multi.s.00\n",
      "multi.s.00\n",
      "post.s.00\n",
      "semi.s.00\n",
      "depressing.s.00\n",
      "semi.s.00\n",
      "depressing.s.00\n",
      "finest.s.00\n",
      "consist_of.v.00\n",
      "non.s.00\n",
      "pre.s.00\n",
      "iced.s.00\n",
      "smart.s.0;2\n",
      "talented.s.00\n",
      "pilot.s.00\n",
      "compassionate.s.00\n",
      "pre.s.00\n",
      "spark.n.2;1\n",
      "anti.s.00\n",
      "anti.s.00\n",
      "hills.n.00\n",
      "finest.s.00\n",
      "biggest.s.00\n",
      "biggest.s.00\n",
      "biggest.s.00\n",
      "hearted.a.00\n",
      "goods.n.00\n",
      "hearted.a.00\n",
      "tinted.s.00\n",
      "multi.s.00\n",
      "talented.s.00\n",
      "post.s.00\n",
      "post.s.00\n",
      "talented.s.00\n",
      "talented.s.00\n",
      "talented.s.00\n",
      "spoil.v.3;1\n",
      "post.s.00\n",
      "avenging.s.00\n",
      "depressing.s.00\n",
      "retail.a.00\n",
      "post.s.00\n",
      "haired.s.00\n",
      "consist_of.v.00\n",
      "non.s.00\n",
      "pre.s.00\n",
      "amoral.a.00\n",
      "multi.s.00\n",
      "anti.s.00\n",
      "permeated.a.00\n",
      "lingering.s.00\n",
      "growing.s.00\n",
      "lingering.s.00\n",
      "biggest.s.00\n",
      "multi.s.00\n",
      "highest.s.00\n",
      "biggest.s.00\n",
      "non.s.00\n",
      "largest.s.00\n",
      "spoil.v.3;1\n",
      "tinted.s.00\n",
      "tinted.s.00\n",
      "post.s.00\n",
      "pilot.s.00\n",
      "talented.s.00\n",
      "biggest.s.00\n",
      "amoral.a.00\n",
      "biggest.s.00\n",
      "biggest.s.00\n",
      "multi.s.00\n",
      "semi.s.00\n",
      "spark.n.2;1\n",
      "lingering.s.00\n",
      "smallest.s.00\n",
      "biggest.s.00\n",
      "consist_of.v.00\n",
      "consist_of.v.00\n",
      "consist_of.v.00\n",
      "consist_of.v.00\n",
      "non.s.00\n",
      "ish.s.00\n",
      "finest.s.00\n",
      "oversimplified.s.00\n",
      "finest.s.00\n",
      "hearted.a.00\n",
      "hearted.a.00\n",
      "anti.s.00\n",
      "largest.s.00\n",
      "rival.s.00\n",
      "rival.s.00\n",
      "semi.s.00\n",
      "hearted.a.00\n",
      "bordering.s.00\n",
      "ish.s.00\n",
      "sport.n.2;1\n",
      "growing.s.00\n",
      "pilot.s.00\n",
      "highest.s.00\n",
      "anti.s.00\n",
      "finest.s.00\n",
      "pumps.n.00\n",
      "bordering.s.00\n",
      "sport.n.2;1\n",
      "largest.s.00\n",
      "smart.s.0;2\n",
      "tuned.s.00\n",
      "pilot.s.00\n",
      "highest.s.00\n",
      "hills.n.00\n",
      "highest.s.00\n",
      "non.s.00\n",
      "sweaty.s.00\n",
      "scarce.s.00\n",
      "furnishings.n.00\n",
      "multi.s.00\n",
      "biggest.s.00\n",
      "bloated.s.00\n",
      "adroit.s.00\n",
      "lower.s.00\n",
      "depressing.s.00\n",
      "sweaty.s.00\n",
      "goods.n.00\n",
      "slightest.s.00\n",
      "bordering.s.00\n",
      "pre.s.00\n",
      "lower.s.00\n",
      "whining.s.00\n",
      "unbearable.s.00\n",
      "slightest.s.00\n",
      "biggest.s.00\n",
      "talented.s.00\n",
      "growing.s.00\n",
      "hills.n.00\n",
      "sweaty.s.00\n",
      "sport.n.2;1\n",
      "semi.s.00\n",
      "hearted.a.00\n",
      "gifted.s.00\n",
      "sway.v.0;1\n",
      "bamboo.s.00\n",
      "post.s.00\n",
      "consist_of.v.00\n",
      "multi.s.00\n",
      "talented.s.00\n",
      "non.s.00\n",
      "shimmering.s.00\n",
      "consist_of.v.00\n",
      "multi.s.00\n",
      "biggest.s.00\n",
      "biggest.s.00\n",
      "talented.s.00\n",
      "talented.s.00\n",
      "powered.s.00\n",
      "spark.n.2;1\n",
      "biggest.s.00\n",
      "talented.s.00\n",
      "depressing.s.00\n",
      "non.s.00\n",
      "pre.s.00\n",
      "depressing.s.00\n",
      "smallest.s.00\n",
      "talented.s.00\n",
      "bloated.s.00\n",
      "incite.v.2;1\n",
      "non.s.00\n",
      "anti.s.00\n",
      "wrinkled.s.00\n",
      "growing.s.00\n",
      "talented.s.00\n",
      "whining.s.00\n",
      "biggest.s.00\n",
      "anti.s.00\n",
      "talented.s.00\n",
      "consonant.n.1;2\n",
      "biggest.s.00\n",
      "semi.s.00\n",
      "gifted.s.00\n",
      "finest.s.00\n",
      "onward.s.00\n",
      "rival.s.00\n",
      "finest.s.00\n",
      "haired.s.00\n",
      "depressing.s.00\n",
      "slightest.s.00\n",
      "post.s.00\n",
      "non.s.00\n",
      "growing.s.00\n",
      "non.s.00\n",
      "handcuffs.n.00\n",
      "rival.s.00\n",
      "rival.s.00\n",
      "non.s.00\n",
      "pilot.s.00\n",
      "growing.s.00\n",
      "smart.s.0;2\n",
      "troop.n.2;1\n",
      "egocentric.s.00\n",
      "goods.n.00\n",
      "goods.n.00\n",
      "smart.s.0;2\n",
      "anti.s.00\n",
      "smart.s.0;2\n",
      "goods.n.00\n",
      "biggest.s.00\n",
      "hearted.a.00\n",
      "depressing.s.00\n",
      "talented.s.00\n",
      "talented.s.00\n",
      "talented.s.00\n",
      "slightest.s.00\n",
      "finest.s.00\n",
      "pilot.s.00\n",
      "post.s.00\n",
      "millions.n.00\n",
      "goods.n.00\n",
      "post.s.00\n",
      "walled.a.00\n",
      "finest.s.00\n",
      "smallest.s.00\n",
      "smart.s.0;2\n",
      "sandy.s.00\n",
      "non.s.00\n",
      "sport.n.2;1\n",
      "spark.n.2;1\n",
      "biggest.s.00\n",
      "docked.a.00\n",
      "semi.s.00\n",
      "onward.s.00\n",
      "rival.s.00\n",
      "pilot.s.00\n",
      "finest.s.00\n",
      "pajamas.n.00\n",
      "finest.s.00\n",
      "anti.s.00\n",
      "post.s.00\n",
      "post.s.00\n",
      "anti.s.00\n",
      "infested.s.00\n",
      "pilot.s.00\n",
      "pilot.s.00\n",
      "consist_of.v.00\n",
      "biggest.s.00\n",
      "haired.s.00\n",
      "spoil.v.3;1\n",
      "non.s.00\n",
      "peeling.s.00\n",
      "ish.s.00\n",
      "non.s.00\n",
      "hearted.a.00\n",
      "ish.s.00\n",
      "semi.s.00\n",
      "glorify.v.2;1\n",
      "worshiping.a.00\n",
      "hearted.a.00\n",
      "hills.n.00\n",
      "talented.s.00\n",
      "depressing.s.00\n",
      "rival.s.00\n",
      "semi.s.00\n",
      "wares.n.00\n",
      "soaking.s.00\n",
      "anti.s.00\n",
      "haired.s.00\n",
      "haired.s.00\n",
      "growing.s.00\n",
      "talented.s.00\n",
      "non.s.00\n",
      "compassionate.s.00\n",
      "biggest.s.00\n",
      "gloves.n.00\n",
      "rival.s.00\n",
      "biggest.s.00\n",
      "talented.s.00\n",
      "biggest.s.00\n",
      "anti.s.00\n",
      "compassionate.s.00\n",
      "hearted.a.00\n",
      "finest.s.00\n",
      "depressing.s.00\n",
      "non.s.00\n",
      "talented.s.00\n",
      "consist_of.v.00\n",
      "growing.s.00\n",
      "spark.n.2;1\n",
      "consist_of.v.00\n",
      "consist_of.v.00\n",
      "finest.s.00\n",
      "consist_of.v.00\n",
      "lower.s.00\n",
      "quivering.s.00\n",
      "pre.s.00\n",
      "pilot.s.00\n",
      "hills.n.00\n",
      "smart.s.0;2\n",
      "biggest.s.00\n",
      "semi.s.00\n",
      "hills.n.00\n",
      "biggest.s.00\n",
      "biggest.s.00\n",
      "anti.s.00\n",
      "lingering.s.00\n",
      "consist_of.v.00\n",
      "smart.s.0;2\n",
      "consist_of.v.00\n",
      "finest.s.00\n",
      "pilot.s.00\n",
      "multi.s.00\n",
      "semi.s.00\n",
      "anti.s.00\n",
      "consist_of.v.00\n"
     ]
    }
   ],
   "source": [
    "new_test_synsets = []\n",
    "\n",
    "for opinion in test_synsets:\n",
    "    new_opinion = []\n",
    "    for sentence in opinion:\n",
    "        filter_sentence = [name for name in sentence if name != \"NE\" and name.split('.')[1] in allowed]\n",
    "        new_filter_sentence = []\n",
    "        for a in filter_sentence:\n",
    "            if \"Lemma('\" in a :\n",
    "                n = a.replace(\"Lemma('\", \"\").replace(\"')\", \"\")\n",
    "                n = n.split(\".\")\n",
    "                n.pop(-1)\n",
    "                n = \".\".join(n)\n",
    "            else:\n",
    "                n = a\n",
    "            try:\n",
    "                get_sentiment(n)\n",
    "                new_filter_sentence.append(n)\n",
    "            except:\n",
    "                print(n)\n",
    "                pass\n",
    "            \n",
    "        filter_sentence = new_filter_sentence\n",
    "        new_opinion.append(filter_sentence)\n",
    "    new_test_synsets.append(new_opinion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/freq_test_synsets.json', 'w') as file:\n",
    "    json.dump(new_test_synsets, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/lesk_test_synsets_lem.json', 'r') as file:\n",
    "    test_synsets = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed = [ \"n\",\"r\", \"a\",\"s\", \"v\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.634"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_opinions = []\n",
    "for opinion in test_synsets:\n",
    "    scores_sentences = []\n",
    "    for sentence in opinion:\n",
    "        filter_sentence = [name for name in sentence if name.split('.')[1] in allowed]\n",
    "\n",
    "        scores_sentences.append(score_synsets(synsets=filter_sentence, score='dif', merge_scores='sum'))\n",
    "\n",
    "    scores_opinions.append(np.mean(scores_sentences))\n",
    "\n",
    "results_opinions = discretize_scores(scores=scores_opinions, threshold=0.06)\n",
    "\n",
    "accuracy_score(y_test, results_opinions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "results = []\n",
    "scores_obj = []\n",
    "scores_res = []\n",
    "for opinion in test_synsets:\n",
    "    total_pos = 0\n",
    "    total_neg = 0\n",
    "    total_obj = 0\n",
    "    for sentence in opinion:\n",
    "        filter_sentence = [name for name in sentence if name.split('.')[1] in allowed]\n",
    "        scores = [get_sentiment(syn) for syn in filter_sentence if get_sentiment(syn) != None]\n",
    "        if len(scores) > 0:\n",
    "            total_pos += sum(s[0] for s in scores if s[0] > 0.5) / len(scores)\n",
    "            total_neg += sum(s[1] for s in scores if s[1] > 0.5) /len(scores)\n",
    "            total_obj += sum(s[2] for s in scores) /len(scores)\n",
    "    score = total_obj\n",
    "    scores_obj.append(total_obj)\n",
    "    scores_res.append(total_pos - total_neg)\n",
    "    if score > 0.15:\n",
    "        # print(\"Positive\")\n",
    "        results.append(1)\n",
    "    elif score < 0.15:\n",
    "        # print(\"Negative\")\n",
    "        results.append(0)\n",
    "    else:\n",
    "        # print(\"Neutral\")\n",
    "        results.append(0)\n",
    "results = [0 if a < 0  else 1 for a in scores_res]\n",
    "print(accuracy_score(y_test, results))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other approaches that can be also useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "scores_opinions = []\n",
    "for opinion in X_test:\n",
    "    scores_sentences = []\n",
    "    for sentence in sent_tokenize(opinion):\n",
    "        scores = analyzer.polarity_scores(sentence)\n",
    "        resta = scores[\"pos\"] - scores[\"neg\"]\n",
    "        scores_sentences.append(resta)\n",
    "    scores_opinions.append(np.mean(scores_sentences))\n",
    "\n",
    "results_opinions = discretize_scores(scores=scores_opinions, threshold=0.06)\n",
    "\n",
    "accuracy_score(y_test, results_opinions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.668"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_opinions = discretize_scores(scores=scores_opinions, threshold=0.015)\n",
    "accuracy_score(y_test, results_opinions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
