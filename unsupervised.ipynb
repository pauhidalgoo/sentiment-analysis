{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SENTIMENT ANALYSIS - UNSUPERVISED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "from textserver import TextServer\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('login.env')\n",
    "ts_password = os.getenv(\"PASSWORD_CAI\")\n",
    "ts_user = os.getenv(\"USER_CAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets    \n",
    "with open('./data/X_test.json', 'r') as file:\n",
    "    X_test = json.load(file)\n",
    "    \n",
    "with open('./data/y_test.json', 'r') as file:\n",
    "    y_test = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TextServer(ts_user, ts_password, 'senses') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "example_sent = \"i guess that if a very wild bachelor party had gone really bad , there would be broken furniture , traces of smack and cocaine on the floor , and a dead prostitute in the bathroom . i guess that if a movie had also gone really bad , there might be the same elements present . coincidence ? poor kyle ( a meek looking jon favreau ) . . . he is about to marry his radiant fiancee , laura ( cameron diaz ) . but before he exchanges his vows , he embarks to las vegas with his friends for one last blowout . but this bachelor party has gone about as bad as it could possibly get . the prostitute has met a horrible , though accidental death , and drugs are everywhere . the five friends agree that there is enough bad evidence here that will send them to jail for a very long time . a surprisingly calm robert boyd ( christian slater ) , who looks like he was groomed to make nefarious decisions , ponders their dilemma for a few minutes before deciding that the best thing to do is to bury the body in the desert where she ' ll never be found . although they stomach the gruesome deed of getting rid of the body ( which also disturbingly involves dismantling the body using power saws in order to stuff it into suitcases ) , when they return from their trip , guilt and paranoia begins to set in which slowly consumes some of the five friends . one is adam ( daniel stern ) he grows increasingly agitated . whenever people look at his van or whenever a cop glances his way , his blood pressure increases . or that just may be because of his dysfunctional family . another is michael , who was actually responsible for her death . he tries to bury his feelings , but the burden of guilt begins to affect his judgment as well . boyd is the ? doer ' of the group . seemingly suffering from a long psychosis , when he feels as if his secret is about to be exposed , he is apt to take extreme measures to cover up his tracks . kyle just hopes that his wedding will live up to laura ' s demanding expectations . then , there ' s moore ( leland orser ) who speaks 5 lines and walks around with a puzzled look on his face . the problem with this reprehensible movie is that it wants to be a cruel comedy , but it presents things in a manner that just aren ' t funny . drugs , mutilation , and killing your own friends isn ' t something to be laughed at . as a straight psychological drama , i could see how it might have worked , as each one tried to maneuver and overcome the weight of their own guilt in their own sometimes - sick ways . but this movie insults us by assuming that we could simply discard our values for 2 hours . if you do like this movie , i don ' t think that i want to know you . i did find slater a convincing leader who sways his friends to choose not the right thing but the ? smart play . ' and diaz adds some brightness to this film as a wedding - needing fiancee . but her talents are essentially wasted here . it ' s obvious that the film maker is trying to strike a certain tone . but the way that he chooses to do it is tasteless . do not make a very bad decision by seeing this film \"\n",
    "\n",
    "sent_text = nltk.sent_tokenize(example_sent)\n",
    "sentences = []\n",
    "no_stopwords_sentences = []\n",
    "for sentence in sent_text:\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    no_stopwords_sentences.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(text:str, remove_stopwords:bool = False) -> None:\n",
    "    sent_list = nltk.sent_tokenize(text)\n",
    "    if remove_stopwords:\n",
    "        no_stopwords_sentences = []\n",
    "        for sentence in sent_text:\n",
    "            word_tokens = word_tokenize(sentence)\n",
    "\n",
    "            filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "            filtered_sentence = []\n",
    "            for w in word_tokens:\n",
    "                if w not in stop_words:\n",
    "                    filtered_sentence.append(w)\n",
    "            no_stopwords_sentences.append(filtered_sentence)\n",
    "        return no_stopwords_sentences\n",
    "    else:\n",
    "        return sent_list\n",
    "\n",
    "def get_synsets(sentences:list, ts:'TextServer' = ts) -> list:\n",
    "    r = []\n",
    "    for sent in sentences:\n",
    "        a = ts.senses(sent)\n",
    "        r.append(a)\n",
    "    return r\n",
    "\n",
    "def get_lesk_synsets(text:str, lemmatize:bool = False, remove_stopwords:bool = False):\n",
    "    tokens = word_tokenize(text)\n",
    "    if lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    if remove_stopwords:\n",
    "        tokens = [w for w in tokens if not w.lower() in stop_words]\n",
    "    tagged_tokens = [(a.text, a.pos_) for a in nlp(text)]\n",
    "    words = []\n",
    "    for token, pos in tagged_tokens:\n",
    "        if pos == \"NOUN\":\n",
    "            syn = lesk(tokens, token, pos=\"n\")\n",
    "        elif pos == \"ADJ\":\n",
    "            syn = lesk(tokens, token, pos=\"a\")\n",
    "        elif pos == \"ADV\":\n",
    "            syn = lesk(tokens, token, pos=\"r\")\n",
    "        elif pos == \"VERB\":\n",
    "            syn = lesk(tokens, token, pos=\"v\")\n",
    "        else:\n",
    "            syn = None\n",
    "        if syn is not None:\n",
    "            words.append(lesk(tokens, token)) if lesk(tokens, token) is not None else None\n",
    "    return words\n",
    "\n",
    "def get_lesk_all_synsets(sentences:list) -> list:\n",
    "    all = []\n",
    "    for sentence in sentences:\n",
    "        all.append(get_lesk_synsets(sentence))\n",
    "    return all\n",
    "    \n",
    "def all_synsets():\n",
    "    for opinion in X_test:\n",
    "        s = get_sentences(opinion)\n",
    "        syns = get_synsets(s)\n",
    "\n",
    "def get_sentiment(synset:'Synset'):\n",
    "    sentiment = swn.senti_synset(synset.name())\n",
    "    return (sentiment.pos_score(), sentiment.neg_score()) if sentiment else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = get_sentences(X_test[0])\n",
    "print(s)\n",
    "syns = get_lesk_all_synsets(s)\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for opinion in X_test:\n",
    "    s = get_sentences(opinion)\n",
    "    syns = get_lesk_all_synsets(s)\n",
    "    total_pos = 0\n",
    "    total_neg = 0\n",
    "    for sentence in syns:\n",
    "        scores = [get_sentiment(syn) for syn in sentence if get_sentiment(syn) != None]\n",
    "        total_pos += sum(s[0] for s in scores)\n",
    "        total_neg += sum(s[1] for s in scores)\n",
    "    if total_pos > total_neg:\n",
    "        # print(\"Positive\")\n",
    "        results.append(1)\n",
    "    elif total_pos < total_neg:\n",
    "        # print(\"Negative\")\n",
    "        results.append(0)\n",
    "    else:\n",
    "        # print(\"Neutral\")\n",
    "        results.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, results))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
