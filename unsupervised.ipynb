{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SENTIMENT ANALYSIS - UNSUPERVISED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Cai Selvas Sala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "from textserver import TextServer\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Optional\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('login.env')\n",
    "ts_password = os.getenv(\"PASSWORD_PAU\")\n",
    "ts_user = os.getenv(\"USER_PAU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets    \n",
    "with open('./data/X_test.json', 'r') as file:\n",
    "    X_test = json.load(file)\n",
    "    \n",
    "with open('./data/y_test.json', 'r') as file:\n",
    "    y_test = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TextServer(ts_user, ts_password, 'senses') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "example_sent = \"i guess that if a very wild bachelor party had gone really bad , there would be broken furniture , traces of smack and cocaine on the floor , and a dead prostitute in the bathroom . i guess that if a movie had also gone really bad , there might be the same elements present . coincidence ? poor kyle ( a meek looking jon favreau ) . . . he is about to marry his radiant fiancee , laura ( cameron diaz ) . but before he exchanges his vows , he embarks to las vegas with his friends for one last blowout . but this bachelor party has gone about as bad as it could possibly get . the prostitute has met a horrible , though accidental death , and drugs are everywhere . the five friends agree that there is enough bad evidence here that will send them to jail for a very long time . a surprisingly calm robert boyd ( christian slater ) , who looks like he was groomed to make nefarious decisions , ponders their dilemma for a few minutes before deciding that the best thing to do is to bury the body in the desert where she ' ll never be found . although they stomach the gruesome deed of getting rid of the body ( which also disturbingly involves dismantling the body using power saws in order to stuff it into suitcases ) , when they return from their trip , guilt and paranoia begins to set in which slowly consumes some of the five friends . one is adam ( daniel stern ) he grows increasingly agitated . whenever people look at his van or whenever a cop glances his way , his blood pressure increases . or that just may be because of his dysfunctional family . another is michael , who was actually responsible for her death . he tries to bury his feelings , but the burden of guilt begins to affect his judgment as well . boyd is the ? doer ' of the group . seemingly suffering from a long psychosis , when he feels as if his secret is about to be exposed , he is apt to take extreme measures to cover up his tracks . kyle just hopes that his wedding will live up to laura ' s demanding expectations . then , there ' s moore ( leland orser ) who speaks 5 lines and walks around with a puzzled look on his face . the problem with this reprehensible movie is that it wants to be a cruel comedy , but it presents things in a manner that just aren ' t funny . drugs , mutilation , and killing your own friends isn ' t something to be laughed at . as a straight psychological drama , i could see how it might have worked , as each one tried to maneuver and overcome the weight of their own guilt in their own sometimes - sick ways . but this movie insults us by assuming that we could simply discard our values for 2 hours . if you do like this movie , i don ' t think that i want to know you . i did find slater a convincing leader who sways his friends to choose not the right thing but the ? smart play . ' and diaz adds some brightness to this film as a wedding - needing fiancee . but her talents are essentially wasted here . it ' s obvious that the film maker is trying to strike a certain tone . but the way that he chooses to do it is tasteless . do not make a very bad decision by seeing this film \"\n",
    "\n",
    "sent_text = nltk.sent_tokenize(example_sent)\n",
    "sentences = []\n",
    "no_stopwords_sentences = []\n",
    "for sentence in sent_text:\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    no_stopwords_sentences.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(text:str, remove_stopwords:bool = False) -> None:\n",
    "    sent_list = nltk.sent_tokenize(text)\n",
    "    if remove_stopwords:\n",
    "        no_stopwords_sentences = []\n",
    "        for sentence in sent_text:\n",
    "            word_tokens = word_tokenize(sentence)\n",
    "\n",
    "            filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "            filtered_sentence = []\n",
    "            for w in word_tokens:\n",
    "                if w not in stop_words:\n",
    "                    filtered_sentence.append(w)\n",
    "            no_stopwords_sentences.append(filtered_sentence)\n",
    "        return no_stopwords_sentences\n",
    "    else:\n",
    "        return sent_list\n",
    "\n",
    "def get_lesk_synsets(text:str, lemmatize:bool = True, remove_stopwords:bool = False):\n",
    "    tokens = word_tokenize(text)\n",
    "    if lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    if remove_stopwords:\n",
    "        tokens = [w for w in tokens if not w.lower() in stop_words]\n",
    "    tagged_tokens = [(a.text, a.pos_) for a in nlp(text)]\n",
    "    words = []\n",
    "    for token, pos in tagged_tokens:\n",
    "        if pos == \"NOUN\":\n",
    "            syn = lesk(tokens, token, pos=\"n\")\n",
    "        elif pos == \"ADJ\":\n",
    "            syn = lesk(tokens, token, pos=\"a\")\n",
    "        elif pos == \"ADV\":\n",
    "            syn = lesk(tokens, token, pos=\"r\")\n",
    "        elif pos == \"VERB\":\n",
    "            syn = lesk(tokens, token, pos=\"v\")\n",
    "        else:\n",
    "            syn = None\n",
    "        if syn is not None:\n",
    "            words.append(syn)\n",
    "    return words\n",
    "\n",
    "def get_lesk_all_synsets(sentences:list, lemmatizer:bool = True) -> list:\n",
    "    all = []\n",
    "    for sentence in sentences:\n",
    "        all.append(get_lesk_synsets(sentence, lemmatizer))\n",
    "    return all\n",
    "    \n",
    "def all_synsets():\n",
    "    all_synsets = []\n",
    "    for opinion in X_test:\n",
    "        s = get_sentences(opinion)\n",
    "        syns = get_synsets(s)\n",
    "        all_synsets.append(syns)\n",
    "        with open('./data/ts_test_synsets.json', 'w') as file:\n",
    "            json.dump(all_synsets, file)\n",
    "\n",
    "def get_sentiment(synset:'Synset'):\n",
    "    sentiment = swn.senti_synset(synset)\n",
    "    return (sentiment.pos_score(), sentiment.neg_score(), sentiment.obj_score()) if sentiment else None\n",
    "\n",
    "def score_synsets(synsets:list, score:str = 'obj', threshold:float = 0.25, merge_scores:str = 'mean') -> float:\n",
    "    \"\"\"\n",
    "    Compute a score for each synset in a list of synsets and merge them into a single score.\n",
    "\n",
    "    Parameters:\n",
    "        synsets (list): List of synsets.\n",
    "        score (str): Score to compute. One of 'pos', 'neg', 'obj', 'max_score', 'dif', 'dif2', 'dif_threshold', 'dif2_threshold', 'dif_obj', 'dif2_obj'.\n",
    "        threshold (float): Threshold for 'dif_threshold' and 'dif2_threshold' scores.\n",
    "        merge_scores (str): Method for merging scores into a single score. One of 'sum', 'mean', 'max', 'min', 'scale', 'scale_norm1_mean', 'scale_norm2_mean'.\n",
    "\n",
    "    Returns:\n",
    "        float: Merged score.\n",
    "    \"\"\"\n",
    "\n",
    "    if score == 'max_score' and merge_scores not in ['sum', 'mean']:\n",
    "        warnings.warn(f\"Score 'max_score' is not compatible with '{merge_scores}'. Using 'sum' instead.\", SyntaxWarning)\n",
    "        merge_scores = 'sum'\n",
    "\n",
    "\n",
    "    dict_score = {\n",
    "        'pos': lambda s: s[0],\n",
    "        'neg': lambda s: s[1], \n",
    "        'obj': lambda s: s[2],\n",
    "        'max_score': lambda s: (-1 if s[0] > s[1] else 1) if s[0] != s[1] else 0,\n",
    "        'dif': lambda s: s[0] - s[1],\n",
    "        'dif2': lambda s: s[0]**2 - s[1]**2,\n",
    "        'dif_threshold': lambda s: (s[0] if s[0] > threshold else 0) - (s[1] if s[1] > threshold else 0),\n",
    "        'dif2_threshold': lambda s: (s[0]**2 if s[0] > threshold else 0) - (s[1]**2 if s[1] > threshold else 0),\n",
    "        'dif_obj': lambda s: (s[0] - s[1]) * s[2],\n",
    "        'dif2_obj': lambda s: (s[0]**2 - s[1]**2) * s[2]\n",
    "        }\n",
    "    \n",
    "    assert score in dict_score.keys(), f\"Score '{score}' not valid. Choose one of {list(dict_score.keys())}\"\n",
    "    \n",
    "    def min_max_scale(scores:list) -> list:\n",
    "        min_score = min(scores)\n",
    "        max_score = max(scores)\n",
    "        return [(s - min_score) / (max_score - min_score) for s in scores]\n",
    "\n",
    "    dict_merge = {\n",
    "        'sum': lambda sc: sum(sc),\n",
    "        'mean': lambda sc: np.mean(sc),\n",
    "        'max': lambda sc: max(sc),\n",
    "        'min': lambda sc: min(sc),\n",
    "        'scale_norm1_mean': lambda sc: np.mean(np.abs(min_max_scale(sc))),\n",
    "        'scale_norm2_mean': lambda sc: np.linalg.norm(min_max_scale(sc)) / len(sc),\n",
    "    }\n",
    "    \n",
    "    assert merge_scores in dict_merge.keys(), f\"Merge score '{merge_scores}' not valid. Choose one of {list(dict_merge.keys())}\"\n",
    "\n",
    "    score_func = dict_score[score]\n",
    "    scores = [score_func(get_sentiment(synset=s)) for s in synsets if s is not None]\n",
    "\n",
    "    merge_func = dict_merge[merge_scores]\n",
    "    scores_merged = merge_func(scores)\n",
    "\n",
    "    return scores_merged\n",
    "\n",
    "def discretize_scores(scores:list, threshold:float, positive_value = 1, negative_value = 0) -> list:\n",
    "    \"\"\"\n",
    "    Returns a list of binary values based on a threshold.\n",
    "\n",
    "    Parameters:\n",
    "        scores (list): List of scores.\n",
    "        threshold (float): Minimum value to consider a score as positive.\n",
    "        positive_value: Value to assign to positive scores.\n",
    "        negative_value: Value to assign to negative scores.\n",
    "    \"\"\"\n",
    "    return list(map(lambda x: positive_value if x >= threshold else negative_value, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i didn ' t realize how apt the name of this movie was until i called the mpaa ( the motion picture association of america - the folks who decide what ' s g , nc 17 , pg , r or x ) to ask why the preview was rated r .\n"
     ]
    }
   ],
   "source": [
    "print(get_sentences(X_test[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = get_sentences(X_test[0])\n",
    "print(s)\n",
    "syns = get_lesk_all_synsets(s)\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def fix_text(text):\n",
    "    fixed_text = re.sub(r'[\\\\\"\\+\\/]', '', text)\n",
    "    fixed_text = re.sub(r'\\s*([.,!\\?;:])\\s*', r'\\1 ', fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' s\\s+', \"'s \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' t\\s+', \"'t \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' re\\s+', \"'re \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' ve\\s+', \"'ve \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' ll\\s+', \"'ll \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+', ' ', fixed_text)\n",
    "    fixed_text = re.sub(r'\\si\\s', ' I ', fixed_text)\n",
    "    fixed_text = re.sub(r'(?:^|(?<=[.!?]))\\s*(\\w)', lambda x: x.group(1).upper(), fixed_text)\n",
    "    return fixed_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_synsets = []\n",
    "for opinion in X_test:\n",
    "    s = get_sentences(opinion)\n",
    "    #s = [fix_text(t) for t in s]\n",
    "    syns = get_lesk_all_synsets(s, lemmatizer=False)\n",
    "    names = [[syn.name() for syn in ll] for ll in syns]\n",
    "    test_synsets.append(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/lesk_test_synsets.json', 'w') as file:\n",
    "    json.dump(test_synsets, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ukb2_test_synsets.json', 'r') as file:\n",
    "    test_synsets = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed = [\"v\", \"a\", \"s\", \"n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "scores_obj = []\n",
    "scores_res = []\n",
    "for opinion in test_synsets:\n",
    "    total_pos = 0\n",
    "    total_neg = 0\n",
    "    total_obj = 0\n",
    "    for sentence in opinion:\n",
    "        filter_sentence = [name for name in sentence if name.split('.')[1] in allowed]\n",
    "        scores = [get_sentiment(syn) for syn in filter_sentence if get_sentiment(syn) != None]\n",
    "        if len(scores) > 0:\n",
    "            total_pos += sum(s[0] for s in scores)/len(scores)\n",
    "            total_neg += sum(s[1] for s in scores)/len(scores)\n",
    "            total_obj += sum(s[2] for s in scores) /len(scores)\n",
    "    score = total_obj\n",
    "    scores_obj.append(total_obj)\n",
    "    scores_res.append(total_pos - total_neg)\n",
    "    if score > 0.15:\n",
    "        # print(\"Positive\")\n",
    "        results.append(1)\n",
    "    elif score < 0.15:\n",
    "        # print(\"Negative\")\n",
    "        results.append(0)\n",
    "    else:\n",
    "        # print(\"Neutral\")\n",
    "        results.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67\n"
     ]
    }
   ],
   "source": [
    "results = [0 if a < 0.1 else 1 for a in scores_res]\n",
    "print(accuracy_score(y_test, results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Own implementation of UKB since TextServer didn't allow us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ukb import *\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "try:\n",
    "    ukb_graph = load_ukb_graph(\"ukb_graph.gexf\")\n",
    "except:\n",
    "    print(\"Creating graph...\")\n",
    "    ukb_graph = build_ukb_graph()\n",
    "    nx.write_gexf(ukb_graph, \"ukb_graph.gexf\")\n",
    "\n",
    "ukb = UKB(ukb_graph)\n",
    "def get_ukb_synsets(text:str):\n",
    "    context_words = extract_context_words(text)\n",
    "    disambiguated_senses = ukb.disambiguate_context(context_words, method=2)\n",
    "    return list(disambiguated_senses.values())\n",
    "\n",
    "def get_ukb_all_synsets(sentences:list) -> list:\n",
    "    all = []\n",
    "    for sentence in sentences:\n",
    "        all.append([a for a in get_ukb_synsets(sentence) if a != None])\n",
    "    return all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'name.n.01', 'john': 'toilet.n.01', 'work': 'work.v.01', 'very': 'very.r.01', 'hard': 'hard.r.01', 'google': 'google.n.01'}\n"
     ]
    }
   ],
   "source": [
    "context_words = extract_context_words(\"my name is john and i work very hard at google\")\n",
    "disambiguated_senses = ukb.disambiguate_context(context_words, method=2)\n",
    "print(disambiguated_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\r"
     ]
    }
   ],
   "source": [
    "test_synsets = []\n",
    "for i, opinion in enumerate(X_test):\n",
    "    s = get_sentences(opinion)\n",
    "    #s = [fix_text(t) for t in s]\n",
    "    syns = get_ukb_all_synsets(s)\n",
    "    test_synsets.append(syns)\n",
    "    print(i, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ukb2_test_synsets.json', 'w') as file:\n",
    "    json.dump(test_synsets, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using just the most freqÃ¼ent synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = json.load(open(\"./data/word_sense_frequencies_semcor.json\"))\n",
    "\n",
    "def get_freq_synsets(text:str):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = [(a.text, a.pos_) for a in nlp(text)]\n",
    "    words = []\n",
    "    for token, pos in tagged_tokens:\n",
    "        if token not in frequencies.keys():\n",
    "            syn = None\n",
    "        else:\n",
    "            if pos == \"NOUN\":\n",
    "                syn = max(frequencies[token], key=lambda key: frequencies[token][key])\n",
    "            elif pos == \"ADJ\":\n",
    "                syn = max(frequencies[token], key=lambda key: frequencies[token][key])\n",
    "            elif pos == \"ADV\":\n",
    "                syn = max(frequencies[token], key=lambda key: frequencies[token][key])\n",
    "            elif pos == \"VERB\":\n",
    "                syn = max(frequencies[token], key=lambda key: frequencies[token][key])\n",
    "            else:\n",
    "                syn = None\n",
    "        if syn is not None:\n",
    "            words.append(syn.name() if syn.__class__.__name__ == \"Lemma\" else syn)\n",
    "    return words\n",
    "\n",
    "def get_freq_all_synsets(sentences:list) -> list:\n",
    "    all = []\n",
    "    for sentence in sentences:\n",
    "        all.append([a for a in get_freq_synsets(sentence) if a != None])\n",
    "    return all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\r"
     ]
    }
   ],
   "source": [
    "test_synsets = []\n",
    "for i, opinion in enumerate(X_test):\n",
    "    s = get_sentences(opinion)\n",
    "    #s = [fix_text(t) for t in s]\n",
    "    syns = get_freq_all_synsets(s)\n",
    "    test_synsets.append(syns)\n",
    "    print(i, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/freq_test_synsets.json', 'w') as file:\n",
    "    json.dump(test_synsets, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ukb1_test_synsets.json', 'r') as file:\n",
    "\ttest_synsets = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed = [  \"v\", \"a\", \"s\", \"n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cai Selvas Sala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Cai Selvas Sala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_opinions = []\n",
    "for opinion in test_synsets:\n",
    "    scores_sentences = []\n",
    "    for sentence in opinion:\n",
    "        filter_sentence = [name for name in sentence if name != \"NE\" and name.split('.')[1] in allowed]\n",
    "        new_filter_sentence = []\n",
    "        for a in filter_sentence:\n",
    "            if \"Lemma('\" in a :\n",
    "                n = a.replace(\"Lemma('\", \"\").replace(\"')\", \"\")\n",
    "                n = n.split(\".\")\n",
    "                n.pop(-1)\n",
    "                n = \".\".join(n)\n",
    "                new_filter_sentence.append(n)\n",
    "            else:\n",
    "                new_filter_sentence.append(a)\n",
    "            \n",
    "        filter_sentence = new_filter_sentence\n",
    "\n",
    "        scores_sentences.append(score_synsets(synsets=filter_sentence, score='pos', merge_scores='mean'))\n",
    "\n",
    "    scores_opinions.append(np.mean(scores_sentences))\n",
    "\n",
    "results_opinions = discretize_scores(scores=scores_opinions, threshold=0.5)\n",
    "\n",
    "accuracy_score(y_test, results_opinions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
