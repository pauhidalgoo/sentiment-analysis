{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SENTIMENT ANALYSIS - UNSUPERVISED**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Optional\n",
    "import warnings\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets    \n",
    "with open('./data/original_data/X_test.json', 'r') as file:\n",
    "    X_test = json.load(file)\n",
    "    \n",
    "with open('./data/original_data/y_test.json', 'r') as file:\n",
    "    y_test = json.load(file)\n",
    "\n",
    "with open('./data/original_data/X_val.json', 'r') as file:\n",
    "    X_val = json.load(file)\n",
    "\n",
    "with open('./data/original_data/y_val.json', 'r') as file:\n",
    "    y_val = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Cai Selvas\n",
      "[nltk_data]     Sala\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package universal_tagset to C:\\Users\\Cai\n",
      "[nltk_data]     Selvas Sala\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_text(text:str) -> str:\n",
    "    \"\"\"\n",
    "    Fix text by removing unwanted characters and adding spaces where needed.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): Text to fix.\n",
    "\n",
    "    Returns:\n",
    "        str: Fixed text.\n",
    "    \"\"\"\n",
    "    fixed_text = re.sub(r'[\\\\\"\\+\\/]', '', text)\n",
    "    fixed_text = re.sub(r'\\s*([.,!\\?;:])\\s*', r'\\1 ', fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' s\\s+', \"'s \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' t\\s+', \"'t \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' re\\s+', \"'re \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' ve\\s+', \"'ve \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' ll\\s+', \"'ll \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+', ' ', fixed_text)\n",
    "    fixed_text = re.sub(r'\\si\\s', ' I ', fixed_text)\n",
    "    fixed_text = re.sub(r'(?:^|(?<=[.!?]))\\s*(\\w)', lambda x: x.group(1).upper(), fixed_text)\n",
    "    return fixed_text.strip()\n",
    "\n",
    "def get_sentences(text:str, remove_stopwords:bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Get sentences from a text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): Text to analyze.\n",
    "        remove_stopwords (bool): Whether to remove stopwords or not.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of sentences.\n",
    "    \"\"\"\n",
    "    sent_list = nltk.sent_tokenize(text)\n",
    "    if remove_stopwords:\n",
    "        no_stopwords_sentences = []\n",
    "        for sentence in sent_list:\n",
    "            word_tokens = word_tokenize(sentence)\n",
    "            filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "            filtered_sentence = []\n",
    "            for w in word_tokens:\n",
    "                if w not in stop_words:\n",
    "                    filtered_sentence.append(w)\n",
    "            no_stopwords_sentences.append(filtered_sentence)\n",
    "        return no_stopwords_sentences\n",
    "    else:\n",
    "        return sent_list\n",
    "    \n",
    "\n",
    "def get_sentiment(synset:'Synset') -> Optional[tuple]:\n",
    "    \"\"\"\n",
    "    Get sentiment scores for a synset.\n",
    "\n",
    "    Parameters:\n",
    "        synset (Synset): Synset to analyze.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple with positive, negative and objective scores if synset is found. None otherwise.\n",
    "    \"\"\"\n",
    "    sentiment = swn.senti_synset(synset)\n",
    "    return (sentiment.pos_score(), sentiment.neg_score(), sentiment.obj_score()) if sentiment else None\n",
    "\n",
    "def score_synsets(synsets:list, score:str = 'obj', score_threshold:float = 0, merge_scores:str = 'mean', default:float = 0, hide_warnings:bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Compute a score for each synset in a list of synsets and merge them into a single score.\n",
    "\n",
    "    Parameters:\n",
    "        synsets (list): List of synsets.\n",
    "        score (str): Score to compute. One of 'pos', 'neg', 'obj', 'max_score', 'dif', 'dif2', 'dif_threshold', 'dif2_threshold'.\n",
    "        score_threshold (float): Threshold for 'dif_threshold' and 'dif2_threshold' scores. Scores below this threshold are set to 0.\n",
    "        merge_scores (str): Method for merging scores into a single score. One of 'sum', 'mean', 'max', 'min', 'scale_norm1_mean', 'scale_norm2_mean'.\n",
    "        default (float): Default score to return if synsets is empty.\n",
    "        hide_warnings (bool): Whether to hide warnings or not.\n",
    "        \n",
    "    Returns:\n",
    "        float: Merged score.\n",
    "    \"\"\"\n",
    "    if len(synsets) == 0:\n",
    "        if not hide_warnings:\n",
    "            warnings.warn(f\"Empty synsets list. Returning default score ({default}).\", SyntaxWarning)\n",
    "        return default\n",
    "\n",
    "    if score == 'max_score' and merge_scores not in ['sum', 'mean']:\n",
    "        if not hide_warnings:\n",
    "            warnings.warn(f\"Score 'max_score' is not compatible with '{merge_scores}'. Using 'sum' instead.\", SyntaxWarning)\n",
    "        merge_scores = 'sum'\n",
    "\n",
    "    dict_score = {\n",
    "        'pos': lambda s: s[0],\n",
    "        'neg': lambda s: s[1], \n",
    "        'obj': lambda s: s[2],\n",
    "        'max_score': lambda s: (-1 if s[0] > s[1] else 1) if s[0] != s[1] else 0,\n",
    "        'dif': lambda s: s[0] - s[1],\n",
    "        'dif2': lambda s: s[0]**2 - s[1]**2,\n",
    "        'dif_threshold': lambda s: (s[0] if abs(s[0]) >= score_threshold else 0) - (s[1] if abs(s[1]) >= score_threshold else 0),\n",
    "        'dif2_threshold': lambda s: (s[0]**2 if abs(s[0]) >= score_threshold else 0) - (s[1]**2 if abs(s[1]) >= score_threshold else 0),\n",
    "        }\n",
    "    \n",
    "    assert score in dict_score.keys(), f\"Score '{score}' not valid. Choose one of {list(dict_score.keys())}\"\n",
    "    \n",
    "    def min_max_scale(scores:list[float|int]) -> list[float]:\n",
    "        \"\"\"\n",
    "        Compute min-max scaling of a list of scores.\n",
    "\n",
    "        Parameters:\n",
    "            scores (list): List of scores.\n",
    "\n",
    "        Returns:\n",
    "            list: Scaled scores.\n",
    "        \"\"\"\n",
    "        min_score = min(scores)\n",
    "        max_score = max(scores)\n",
    "        return [(s - min_score) / (max_score - min_score) for s in scores]\n",
    "\n",
    "    dict_merge = {\n",
    "        'sum': lambda sc: sum(sc),\n",
    "        'mean': lambda sc: np.mean(sc),\n",
    "        'max': lambda sc: max(sc),\n",
    "        'min': lambda sc: min(sc),\n",
    "        'scale_norm1_mean': lambda sc: np.mean(np.abs(min_max_scale(sc))),\n",
    "        'scale_norm2_mean': lambda sc: np.linalg.norm(min_max_scale(sc)) / len(sc),\n",
    "    }\n",
    "    \n",
    "    assert merge_scores in dict_merge.keys(), f\"Merge score '{merge_scores}' not valid. Choose one of {list(dict_merge.keys())}\"\n",
    "\n",
    "    score_func = dict_score[score]\n",
    "    scores = [score_func(get_sentiment(synset=s)) for s in synsets if s is not None]\n",
    "\n",
    "    if merge_scores in ['scale_norm1_mean', 'scale_norm2_mean'] and min(scores) == max(scores):\n",
    "        if not hide_warnings:\n",
    "            warnings.warn(f\"Scores are all the same and cannot be scaled. Returning default score ({default}).\", RuntimeWarning)\n",
    "        return default\n",
    "\n",
    "\n",
    "    merge_func = dict_merge[merge_scores]\n",
    "    scores_merged = merge_func(scores)\n",
    "\n",
    "    return scores_merged\n",
    "\n",
    "def discretize_scores(scores:list, threshold:float, positive_value = 1, negative_value = 0) -> list:\n",
    "    \"\"\"\n",
    "    Returns a list of binary values based on a threshold.\n",
    "\n",
    "    Parameters:\n",
    "        scores (list): List of scores.\n",
    "        threshold (float): Minimum value to consider a score as positive.\n",
    "        positive_value: Value to assign to positive scores.\n",
    "        negative_value: Value to assign to negative scores.\n",
    "    \"\"\"\n",
    "    return list(map(lambda x: positive_value if x >= threshold else negative_value, scores))\n",
    "def negativize(sentence):\n",
    "    \"\"\"\n",
    "    Based on https://gist.github.com/UtkarshRedd/3fbfd354ea7a6f83bd8f9419a27b0543\n",
    "    \"\"\"\n",
    "    for i in range(2, len(sentence)):\n",
    "        for distance in [1,2]:\n",
    "            if sentence[i-distance] in ['not', \"n't\", \"t\", \"DELETED\"]:\n",
    "                antonyms = []\n",
    "                for syn in wn.synsets(sentence[i]):\n",
    "                    for l in syn.lemmas():\n",
    "                        if l.antonyms():\n",
    "                            antonyms.append(l.antonyms()[0].name())\n",
    "                max_dissimilarity = 0\n",
    "                antonym_max = None\n",
    "                for ant in antonyms:\n",
    "                    for syn in wn.synsets(ant):\n",
    "                        w1 = wn.synsets(sentence[i])[0].name()\n",
    "                        w2 = syn.name()\n",
    "                        word1 = wn.synset(w1)\n",
    "                        word2 = wn.synset(w2)\n",
    "                        if isinstance(word1.path_similarity(word2), (float, int)):\n",
    "                            temp = 1 - word1.path_similarity(word2)\n",
    "                            if temp > max_dissimilarity:\n",
    "                                max_dissimilarity = temp\n",
    "                                antonym_max = ant\n",
    "                if antonym_max:\n",
    "                    sentence[i] = antonym_max\n",
    "                    sentence[i-distance] = 'DELETED'\n",
    "    sentence = [word for word in sentence if word != 'DELETED']\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LESK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lesk_synsets(text:str, lemmatize:bool = True, remove_stopwords:bool = False):\n",
    "    tokens = word_tokenize(text)\n",
    "    if lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    if remove_stopwords:\n",
    "        tokens = [w for w in tokens if not w.lower() in stop_words]\n",
    "    tagged_tokens = [(a.text, a.pos_) for a in nlp(text)]\n",
    "    words = []\n",
    "    for token, pos in tagged_tokens:\n",
    "        if pos == \"NOUN\":\n",
    "            syn = lesk(tokens, token, pos=\"n\")\n",
    "        elif pos == \"ADJ\":\n",
    "            syn = lesk(tokens, token) # pos \"a\" if not want s\n",
    "        elif pos == \"ADV\":\n",
    "            syn = lesk(tokens, token, pos=\"r\")\n",
    "        elif pos == \"VERB\":\n",
    "            syn = lesk(tokens, token, pos=\"v\")\n",
    "        else:\n",
    "            syn = None\n",
    "        if syn is not None:\n",
    "            words.append(syn)\n",
    "    return words\n",
    "\n",
    "def get_lesk_all_synsets(sentences:list, lemmatizer:bool = True) -> list:\n",
    "    all = []\n",
    "    for sentence in sentences:\n",
    "        all.append(get_lesk_synsets(sentence, lemmatizer))\n",
    "    return all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[544], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m s \u001b[38;5;241m=\u001b[39m get_sentences(opinion)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#s = [fix_text(t) for t in s]\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m syns \u001b[38;5;241m=\u001b[39m \u001b[43mget_lesk_all_synsets\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m names \u001b[38;5;241m=\u001b[39m [[syn\u001b[38;5;241m.\u001b[39mname() \u001b[38;5;28;01mfor\u001b[39;00m syn \u001b[38;5;129;01min\u001b[39;00m ll] \u001b[38;5;28;01mfor\u001b[39;00m ll \u001b[38;5;129;01min\u001b[39;00m syns]\n\u001b[0;32m      7\u001b[0m test_synsets\u001b[38;5;241m.\u001b[39mappend(names)\n",
      "Cell \u001b[1;32mIn[541], line 43\u001b[0m, in \u001b[0;36mget_lesk_all_synsets\u001b[1;34m(sentences, lemmatizer)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mall\u001b[39m \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28mall\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[43mget_lesk_synsets\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemmatizer\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mall\u001b[39m\n",
      "Cell \u001b[1;32mIn[541], line 23\u001b[0m, in \u001b[0;36mget_lesk_synsets\u001b[1;34m(text, lemmatize, remove_stopwords)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_stopwords:\n\u001b[0;32m     22\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m---> 23\u001b[0m tagged_tokens \u001b[38;5;241m=\u001b[39m [(a\u001b[38;5;241m.\u001b[39mtext, a\u001b[38;5;241m.\u001b[39mpos_) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m     24\u001b[0m words \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token, pos \u001b[38;5;129;01min\u001b[39;00m tagged_tokens:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spacy\\language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1049\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\spacy\\pipeline\\tok2vec.py:126\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[1;34m(self, docs)\u001b[0m\n\u001b[0;32m    124\u001b[0m     width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39malloc((\u001b[38;5;241m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[1;32m--> 126\u001b[0m tokvecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\thinc\\model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\thinc\\layers\\with_array.py:42\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, Xseq, is_train)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](Xseq, is_train)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\thinc\\layers\\with_array.py:77\u001b[0m, in \u001b[0;36m_list_forward\u001b[1;34m(model, Xs, is_train)\u001b[0m\n\u001b[0;32m     75\u001b[0m lengths \u001b[38;5;241m=\u001b[39m NUMPY_OPS\u001b[38;5;241m.\u001b[39masarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[0;32m     76\u001b[0m Xf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(Xs, pad\u001b[38;5;241m=\u001b[39mpad)\n\u001b[1;32m---> 77\u001b[0m Yf, get_dXf \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dYs: ListXd) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ListXd:\n\u001b[0;32m     80\u001b[0m     dYf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(dYs, pad\u001b[38;5;241m=\u001b[39mpad)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\thinc\\layers\\residual.py:41\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output \u001b[38;5;241m+\u001b[39m dX\n\u001b[1;32m---> 41\u001b[0m Y, backprop_layer \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [X[i] \u001b[38;5;241m+\u001b[39m Y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))], backprop\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "    \u001b[1;31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\thinc\\layers\\maxout.py:52\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     50\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape2f(W, nO \u001b[38;5;241m*\u001b[39m nP, nI)\n\u001b[1;32m---> 52\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m Y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape1f(b, nO \u001b[38;5;241m*\u001b[39m nP)\n\u001b[0;32m     54\u001b[0m Z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape3f(Y, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], nO, nP)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_synsets = []\n",
    "for opinion in X_test:\n",
    "    s = get_sentences(opinion)\n",
    "    #s = [fix_text(t) for t in s]\n",
    "    syns = get_lesk_all_synsets(s, lemmatizer=False)\n",
    "    names = [[syn.name() for syn in ll] for ll in syns]\n",
    "    test_synsets.append(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/lesk_test_synsets.json', 'w') as file:\n",
    "    json.dump(test_synsets, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_synsets = []\n",
    "for opinion in X_val:\n",
    "    s = get_sentences(opinion)\n",
    "    #s = [fix_text(t) for t in s]\n",
    "    syns = get_lesk_all_synsets(s, lemmatizer=False)\n",
    "    names = [[syn.name() for syn in ll] for ll in syns]\n",
    "    val_synsets.append(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/synsets/lesk_val_synsets.json', 'w') as file:\n",
    "\tjson.dump(val_synsets, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UKB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Own implementation of UKB since TextServer didn't allow us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ukb import *\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "try:\n",
    "    ukb_graph = load_ukb_graph(\"ukb_graph.gexf\")\n",
    "except:\n",
    "    print(\"Creating graph...\")\n",
    "    ukb_graph = build_ukb_graph()\n",
    "    nx.write_gexf(ukb_graph, \"ukb_graph.gexf\")\n",
    "\n",
    "ukb = UKB(ukb_graph)\n",
    "def get_ukb_synsets(text:str):\n",
    "    context_words = extract_context_words(text)\n",
    "    disambiguated_senses = ukb.disambiguate_context(context_words, method=3)\n",
    "    return list(disambiguated_senses.values())\n",
    "\n",
    "def get_ukb_all_synsets(sentences:list) -> list:\n",
    "    all = []\n",
    "    for sentence in sentences:\n",
    "        all.append([a for a in get_ukb_synsets(sentence) if a != None])\n",
    "    return all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'name.n.01', 'john': 'john.n.02', 'work': 'shape.v.02', 'very': 'very.r.01', 'hard': 'hard.r.01', 'google': 'google.n.01'}\n"
     ]
    }
   ],
   "source": [
    "context_words = extract_context_words(\"find the solution to this problem\")\n",
    "disambiguated_senses = ukb.disambiguate_context(context_words, method=3)\n",
    "print(disambiguated_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\r"
     ]
    }
   ],
   "source": [
    "test_synsets = []\n",
    "for i, opinion in enumerate(X_test):\n",
    "    s = get_sentences(opinion)\n",
    "    #s = [fix_text(t) for t in s]\n",
    "    syns = get_ukb_all_synsets(s)\n",
    "    test_synsets.append(syns)\n",
    "    print(i, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ukb2_test_synsets.json', 'w') as file:\n",
    "    json.dump(test_synsets, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_synsets = []\n",
    "for i, opinion in enumerate(X_val):\n",
    "    s = get_sentences(opinion)\n",
    "    #s = [fix_text(t) for t in s]\n",
    "    syns = get_ukb_all_synsets(s)\n",
    "    val_synsets.append(syns)\n",
    "    print(i, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/synsets/ukb2_val_synsets.json', 'w') as file:\n",
    "\tjson.dump(val_synsets, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using just the most freqÃ¼ent synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = json.load(open(\"./data/synsets/word_sense_frequencies_semcor.json\"))\n",
    "\n",
    "def get_freq_synsets(text:str):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = [(a.text, a.pos_) for a in nlp(text)]\n",
    "    words = []\n",
    "    for token, pos in tagged_tokens:\n",
    "        if token not in frequencies.keys():\n",
    "            syn = None\n",
    "        else:\n",
    "            if pos == \"NOUN\":\n",
    "                syn = max(frequencies[token], key=lambda key: frequencies[token][key])\n",
    "            elif pos == \"ADJ\":\n",
    "                syn = max(frequencies[token], key=lambda key: frequencies[token][key])\n",
    "            elif pos == \"ADV\":\n",
    "                syn = max(frequencies[token], key=lambda key: frequencies[token][key])\n",
    "            elif pos == \"VERB\":\n",
    "                syn = max(frequencies[token], key=lambda key: frequencies[token][key])\n",
    "            else:\n",
    "                syn = None\n",
    "        if syn is not None:\n",
    "            words.append(syn.name() if syn.__class__.__name__ == \"Lemma\" else syn)\n",
    "    return words\n",
    "\n",
    "def get_freq_all_synsets(sentences:list) -> list:\n",
    "    all = []\n",
    "    for sentence in sentences:\n",
    "        all.append([a for a in get_freq_synsets(sentence) if a != None])\n",
    "    return all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\r"
     ]
    }
   ],
   "source": [
    "test_synsets = []\n",
    "for i, opinion in enumerate(X_test):\n",
    "    s = get_sentences(opinion)\n",
    "    #s = [fix_text(t) for t in s]\n",
    "    syns = get_freq_all_synsets(s)\n",
    "    test_synsets.append(syns)\n",
    "    print(i, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/synsets/freq_test_synsets.json', 'w') as file:\n",
    "    json.dump(test_synsets, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/synsets/ukb1_test_synsets.json', 'r') as file:\n",
    "\ttest_synsets = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed = [  \"v\", \"a\", \"s\", \"n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gifted.s.00\n",
      "non.s.00\n",
      "condensed.s.00\n",
      "railway.n.1;2\n",
      "anti.s.00\n",
      "post.s.00\n",
      "anti.s.00\n",
      "growing.s.00\n",
      "talented.s.00\n",
      "ish.s.00\n",
      "lingering.s.00\n",
      "hearted.a.00\n",
      "semi.s.00\n",
      "post.s.00\n",
      "post.s.00\n",
      "gifted.s.00\n",
      "post.s.00\n",
      "bloated.s.00\n",
      "touring.s.00\n",
      "growing.s.00\n",
      "anti.s.00\n",
      "anti.s.00\n",
      "spark.n.2;1\n",
      "growing.s.00\n",
      "talented.s.00\n",
      "finest.s.00\n",
      "biggest.s.00\n",
      "talented.s.00\n",
      "bloated.s.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wares.n.00\n",
      "growing.s.00\n",
      "non.s.00\n",
      "talented.s.00\n",
      "guerrilla.s.00\n",
      "depressing.s.00\n",
      "post.s.00\n",
      "anti.s.00\n",
      "tuned.s.00\n",
      "hearted.a.00\n",
      "slightest.s.00\n",
      "spark.n.2;1\n",
      "multi.s.00\n",
      "growing.s.00\n",
      "furnishings.n.00\n",
      "growing.s.00\n",
      "anti.s.00\n",
      "growing.s.00\n",
      "tuned.s.00\n",
      "growing.s.00\n",
      "growing.s.00\n",
      "depressing.s.00\n",
      "sweaty.s.00\n",
      "anti.s.00\n",
      "growing.s.00\n",
      "pre.s.00\n",
      "spark.n.2;1\n",
      "infested.s.00\n",
      "semi.s.00\n",
      "talented.s.00\n",
      "talented.s.00\n",
      "talented.s.00\n",
      "biggest.s.00\n",
      "haired.s.00\n",
      "compassionate.s.00\n",
      "sway.v.0;1\n",
      "unbearable.s.00\n",
      "non.s.00\n",
      "depressing.s.00\n",
      "goods.n.00\n",
      "non.s.00\n",
      "anti.s.00\n",
      "scrap.s.00\n",
      "rotting.s.00\n",
      "smart.s.0;2\n",
      "consist_of.v.00\n",
      "talented.s.00\n",
      "whining.s.00\n",
      "rival.s.00\n",
      "millions.n.00\n",
      "post.s.00\n",
      "semi.s.00\n",
      "calming.s.00\n",
      "growing.s.00\n",
      "sandy.s.00\n",
      "depressing.s.00\n",
      "lingering.s.00\n",
      "lower.s.00\n",
      "post.s.00\n",
      "birthe.v.00\n",
      "lower.s.00\n",
      "rival.s.00\n",
      "spark.n.2;1\n",
      "pre.s.00\n",
      "spark.n.2;1\n",
      "depressing.s.00\n",
      "biggest.s.00\n",
      "multi.s.00\n",
      "semi.s.00\n",
      "non.s.00\n",
      "slightest.s.00\n",
      "growing.s.00\n",
      "consist_of.v.00\n",
      "anti.s.00\n",
      "consist_of.v.00\n",
      "spoil.v.3;1\n",
      "pilot.s.00\n",
      "carping.s.00\n",
      "sunshiny.s.00\n",
      "reserve.s.00\n",
      "post.s.00\n",
      "docked.a.00\n",
      "consist_of.v.00\n",
      "depressing.s.00\n",
      "consist_of.v.00\n",
      "fashions.n.00\n",
      "rival.s.00\n",
      "post.s.00\n",
      "lower.s.00\n",
      "lower.s.00\n",
      "growing.s.00\n",
      "supplies.n.00\n",
      "biggest.s.00\n",
      "hills.n.00\n",
      "non.s.00\n",
      "non.s.00\n",
      "anti.s.00\n",
      "post.s.00\n",
      "sweaty.s.00\n",
      "depressing.s.00\n",
      "pilot.s.00\n",
      "pilot.s.00\n",
      "gloves.n.00\n",
      "pilot.s.00\n",
      "pilot.s.00\n",
      "pre.s.00\n",
      "pilot.s.00\n",
      "unbearable.s.00\n",
      "non.s.00\n",
      "semi.s.00\n",
      "depressing.s.00\n",
      "post.s.00\n",
      "biggest.s.00\n",
      "hills.n.00\n",
      "hills.n.00\n",
      "hills.n.00\n",
      "pilot.s.00\n",
      "pilot.s.00\n",
      "pilot.s.00\n",
      "pilot.s.00\n",
      "pilot.s.00\n",
      "multi.s.00\n",
      "non.s.00\n",
      "semi.s.00\n",
      "talented.s.00\n",
      "biggest.s.00\n",
      "powered.s.00\n",
      "talented.s.00\n",
      "talented.s.00\n",
      "biggest.s.00\n",
      "talented.s.00\n",
      "rival.s.00\n",
      "egocentric.s.00\n",
      "talented.s.00\n",
      "multi.s.00\n",
      "biggest.s.00\n",
      "whining.s.00\n",
      "ish.s.00\n",
      "showy.s.0;1\n",
      "non.s.00\n",
      "festering.s.00\n",
      "depressing.s.00\n",
      "biggest.s.00\n",
      "anti.s.00\n",
      "highest.s.00\n",
      "finest.s.00\n",
      "bloated.s.00\n",
      "pilot.s.00\n",
      "consist_of.v.00\n",
      "harmful.s.00\n",
      "multi.s.00\n",
      "talented.s.00\n",
      "goods.n.00\n",
      "oldest.s.00\n",
      "pre.s.00\n",
      "sport.n.2;1\n",
      "post.s.00\n",
      "post.s.00\n",
      "growing.s.00\n",
      "growing.s.00\n",
      "biggest.s.00\n",
      "consist_of.v.00\n",
      "multi.s.00\n",
      "multi.s.00\n",
      "post.s.00\n",
      "semi.s.00\n",
      "depressing.s.00\n",
      "semi.s.00\n",
      "depressing.s.00\n",
      "finest.s.00\n",
      "consist_of.v.00\n",
      "non.s.00\n",
      "pre.s.00\n",
      "iced.s.00\n",
      "smart.s.0;2\n",
      "talented.s.00\n",
      "pilot.s.00\n",
      "compassionate.s.00\n",
      "pre.s.00\n",
      "spark.n.2;1\n",
      "anti.s.00\n",
      "anti.s.00\n",
      "hills.n.00\n",
      "finest.s.00\n",
      "biggest.s.00\n",
      "biggest.s.00\n",
      "biggest.s.00\n",
      "hearted.a.00\n",
      "goods.n.00\n",
      "hearted.a.00\n",
      "tinted.s.00\n",
      "multi.s.00\n",
      "talented.s.00\n",
      "post.s.00\n",
      "post.s.00\n",
      "talented.s.00\n",
      "talented.s.00\n",
      "talented.s.00\n",
      "spoil.v.3;1\n",
      "post.s.00\n",
      "avenging.s.00\n",
      "depressing.s.00\n",
      "retail.a.00\n",
      "post.s.00\n",
      "haired.s.00\n",
      "consist_of.v.00\n",
      "non.s.00\n",
      "pre.s.00\n",
      "amoral.a.00\n",
      "multi.s.00\n",
      "anti.s.00\n",
      "permeated.a.00\n",
      "lingering.s.00\n",
      "growing.s.00\n",
      "lingering.s.00\n",
      "biggest.s.00\n",
      "multi.s.00\n",
      "highest.s.00\n",
      "biggest.s.00\n",
      "non.s.00\n",
      "largest.s.00\n",
      "spoil.v.3;1\n",
      "tinted.s.00\n",
      "tinted.s.00\n",
      "post.s.00\n",
      "pilot.s.00\n",
      "talented.s.00\n",
      "biggest.s.00\n",
      "amoral.a.00\n",
      "biggest.s.00\n",
      "biggest.s.00\n",
      "multi.s.00\n",
      "semi.s.00\n",
      "spark.n.2;1\n",
      "lingering.s.00\n",
      "smallest.s.00\n",
      "biggest.s.00\n",
      "consist_of.v.00\n",
      "consist_of.v.00\n",
      "consist_of.v.00\n",
      "consist_of.v.00\n",
      "non.s.00\n",
      "ish.s.00\n",
      "finest.s.00\n",
      "oversimplified.s.00\n",
      "finest.s.00\n",
      "hearted.a.00\n",
      "hearted.a.00\n",
      "anti.s.00\n",
      "largest.s.00\n",
      "rival.s.00\n",
      "rival.s.00\n",
      "semi.s.00\n",
      "hearted.a.00\n",
      "bordering.s.00\n",
      "ish.s.00\n",
      "sport.n.2;1\n",
      "growing.s.00\n",
      "pilot.s.00\n",
      "highest.s.00\n",
      "anti.s.00\n",
      "finest.s.00\n",
      "pumps.n.00\n",
      "bordering.s.00\n",
      "sport.n.2;1\n",
      "largest.s.00\n",
      "smart.s.0;2\n",
      "tuned.s.00\n",
      "pilot.s.00\n",
      "highest.s.00\n",
      "hills.n.00\n",
      "highest.s.00\n",
      "non.s.00\n",
      "sweaty.s.00\n",
      "scarce.s.00\n",
      "furnishings.n.00\n",
      "multi.s.00\n",
      "biggest.s.00\n",
      "bloated.s.00\n",
      "adroit.s.00\n",
      "lower.s.00\n",
      "depressing.s.00\n",
      "sweaty.s.00\n",
      "goods.n.00\n",
      "slightest.s.00\n",
      "bordering.s.00\n",
      "pre.s.00\n",
      "lower.s.00\n",
      "whining.s.00\n",
      "unbearable.s.00\n",
      "slightest.s.00\n",
      "biggest.s.00\n",
      "talented.s.00\n",
      "growing.s.00\n",
      "hills.n.00\n",
      "sweaty.s.00\n",
      "sport.n.2;1\n",
      "semi.s.00\n",
      "hearted.a.00\n",
      "gifted.s.00\n",
      "sway.v.0;1\n",
      "bamboo.s.00\n",
      "post.s.00\n",
      "consist_of.v.00\n",
      "multi.s.00\n",
      "talented.s.00\n",
      "non.s.00\n",
      "shimmering.s.00\n",
      "consist_of.v.00\n",
      "multi.s.00\n",
      "biggest.s.00\n",
      "biggest.s.00\n",
      "talented.s.00\n",
      "talented.s.00\n",
      "powered.s.00\n",
      "spark.n.2;1\n",
      "biggest.s.00\n",
      "talented.s.00\n",
      "depressing.s.00\n",
      "non.s.00\n",
      "pre.s.00\n",
      "depressing.s.00\n",
      "smallest.s.00\n",
      "talented.s.00\n",
      "bloated.s.00\n",
      "incite.v.2;1\n",
      "non.s.00\n",
      "anti.s.00\n",
      "wrinkled.s.00\n",
      "growing.s.00\n",
      "talented.s.00\n",
      "whining.s.00\n",
      "biggest.s.00\n",
      "anti.s.00\n",
      "talented.s.00\n",
      "consonant.n.1;2\n",
      "biggest.s.00\n",
      "semi.s.00\n",
      "gifted.s.00\n",
      "finest.s.00\n",
      "onward.s.00\n",
      "rival.s.00\n",
      "finest.s.00\n",
      "haired.s.00\n",
      "depressing.s.00\n",
      "slightest.s.00\n",
      "post.s.00\n",
      "non.s.00\n",
      "growing.s.00\n",
      "non.s.00\n",
      "handcuffs.n.00\n",
      "rival.s.00\n",
      "rival.s.00\n",
      "non.s.00\n",
      "pilot.s.00\n",
      "growing.s.00\n",
      "smart.s.0;2\n",
      "troop.n.2;1\n",
      "egocentric.s.00\n",
      "goods.n.00\n",
      "goods.n.00\n",
      "smart.s.0;2\n",
      "anti.s.00\n",
      "smart.s.0;2\n",
      "goods.n.00\n",
      "biggest.s.00\n",
      "hearted.a.00\n",
      "depressing.s.00\n",
      "talented.s.00\n",
      "talented.s.00\n",
      "talented.s.00\n",
      "slightest.s.00\n",
      "finest.s.00\n",
      "pilot.s.00\n",
      "post.s.00\n",
      "millions.n.00\n",
      "goods.n.00\n",
      "post.s.00\n",
      "walled.a.00\n",
      "finest.s.00\n",
      "smallest.s.00\n",
      "smart.s.0;2\n",
      "sandy.s.00\n",
      "non.s.00\n",
      "sport.n.2;1\n",
      "spark.n.2;1\n",
      "biggest.s.00\n",
      "docked.a.00\n",
      "semi.s.00\n",
      "onward.s.00\n",
      "rival.s.00\n",
      "pilot.s.00\n",
      "finest.s.00\n",
      "pajamas.n.00\n",
      "finest.s.00\n",
      "anti.s.00\n",
      "post.s.00\n",
      "post.s.00\n",
      "anti.s.00\n",
      "infested.s.00\n",
      "pilot.s.00\n",
      "pilot.s.00\n",
      "consist_of.v.00\n",
      "biggest.s.00\n",
      "haired.s.00\n",
      "spoil.v.3;1\n",
      "non.s.00\n",
      "peeling.s.00\n",
      "ish.s.00\n",
      "non.s.00\n",
      "hearted.a.00\n",
      "ish.s.00\n",
      "semi.s.00\n",
      "glorify.v.2;1\n",
      "worshiping.a.00\n",
      "hearted.a.00\n",
      "hills.n.00\n",
      "talented.s.00\n",
      "depressing.s.00\n",
      "rival.s.00\n",
      "semi.s.00\n",
      "wares.n.00\n",
      "soaking.s.00\n",
      "anti.s.00\n",
      "haired.s.00\n",
      "haired.s.00\n",
      "growing.s.00\n",
      "talented.s.00\n",
      "non.s.00\n",
      "compassionate.s.00\n",
      "biggest.s.00\n",
      "gloves.n.00\n",
      "rival.s.00\n",
      "biggest.s.00\n",
      "talented.s.00\n",
      "biggest.s.00\n",
      "anti.s.00\n",
      "compassionate.s.00\n",
      "hearted.a.00\n",
      "finest.s.00\n",
      "depressing.s.00\n",
      "non.s.00\n",
      "talented.s.00\n",
      "consist_of.v.00\n",
      "growing.s.00\n",
      "spark.n.2;1\n",
      "consist_of.v.00\n",
      "consist_of.v.00\n",
      "finest.s.00\n",
      "consist_of.v.00\n",
      "lower.s.00\n",
      "quivering.s.00\n",
      "pre.s.00\n",
      "pilot.s.00\n",
      "hills.n.00\n",
      "smart.s.0;2\n",
      "biggest.s.00\n",
      "semi.s.00\n",
      "hills.n.00\n",
      "biggest.s.00\n",
      "biggest.s.00\n",
      "anti.s.00\n",
      "lingering.s.00\n",
      "consist_of.v.00\n",
      "smart.s.0;2\n",
      "consist_of.v.00\n",
      "finest.s.00\n",
      "pilot.s.00\n",
      "multi.s.00\n",
      "semi.s.00\n",
      "anti.s.00\n",
      "consist_of.v.00\n"
     ]
    }
   ],
   "source": [
    "new_test_synsets = []\n",
    "\n",
    "for opinion in test_synsets:\n",
    "    new_opinion = []\n",
    "    for sentence in opinion:\n",
    "        filter_sentence = [name for name in sentence if name != \"NE\" and name.split('.')[1] in allowed]\n",
    "        new_filter_sentence = []\n",
    "        for a in filter_sentence:\n",
    "            if \"Lemma('\" in a :\n",
    "                n = a.replace(\"Lemma('\", \"\").replace(\"')\", \"\")\n",
    "                n = n.split(\".\")\n",
    "                n.pop(-1)\n",
    "                n = \".\".join(n)\n",
    "            else:\n",
    "                n = a\n",
    "            try:\n",
    "                get_sentiment(n)\n",
    "                new_filter_sentence.append(n)\n",
    "            except:\n",
    "                print(n)\n",
    "                pass\n",
    "            \n",
    "        filter_sentence = new_filter_sentence\n",
    "        new_opinion.append(filter_sentence)\n",
    "    new_test_synsets.append(new_opinion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/synsets/freq_test_synsets.json', 'w') as file:\n",
    "    json.dump(new_test_synsets, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/synsets/lesk_test_s_synsets.json', 'r') as file:\n",
    "    test_synsets = json.load(file)\n",
    "\n",
    "with open('./data/synsets/lesk_val_s_synsets.json', 'r') as file:\n",
    "    val_synsets = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(val_synsets:list, \n",
    "                          y_val:list, \n",
    "                          allowed:list = [\"n\", \"a\", \"s\", \"v\", \"r\"],\n",
    "                          scores:list = ['pos', 'neg', 'obj', 'max_score', 'dif', 'dif2', 'dif_threshold', 'dif2_threshold'],\n",
    "                          scores_thresholds:list = [0.05, 0.1, 0.25, 0.4, 0.6],\n",
    "                          merges:list = ['sum', 'mean', 'max', 'min', 'scale_norm1_mean', 'scale_norm2_mean'],\n",
    "                          thresholds = [-0.6, -0.4, -0.25, -0.1, 0, 0.1, 0.25, 0.4, 0.6]\n",
    "                          ) -> tuple[list[tuple[tuple, tuple]], float, dict]:\n",
    "    \"\"\"\n",
    "    Run a validation experiment with different parameters to find the best combination.\n",
    "\n",
    "    Parameters:\n",
    "        val_synsets (list): List of validation synsets.\n",
    "        y_val (list): List of validation labels.\n",
    "        allowed (list): Allowed POS tags.\n",
    "        scores (list): List of scores to compute.\n",
    "        scores_thresholds (list): List of thresholds for 'dif_threshold' and 'dif2_threshold' scores.\n",
    "        merges (list): List of methods to merge scores.\n",
    "        thresholds (list): List of thresholds for discretization.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Best parameters (list of all combinations with the best accuracy), best accuracy (float) and results (dict of all combinations with their accuracy).\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    allow_combinations = list(itertools.chain.from_iterable(itertools.combinations(allowed, r) for r in range(1, len(allowed)+1)))\n",
    "    results_allow = {tuple(a): dict() for a in allow_combinations} # Dict of allowed combinations. Each allowed combination has a dict of results\n",
    "    iter = 1\n",
    "    total_iters = len(scores) * len(merges) * len(thresholds) * len(allow_combinations)\n",
    "    for score in scores:\n",
    "        for merge in merges:\n",
    "            for thresh in thresholds:\n",
    "                for allow in results_allow.keys():\n",
    "                    print(f\"Running validation: {(iter/total_iters)*100:.2f}%\", end=\"\\r\")\n",
    "                    iter += 1\n",
    "\n",
    "                    if score == 'max_score' and merge not in ['sum', 'mean']:\n",
    "                        results_allow[tuple(allow)][(score, None, merge, thresh)] = float('nan')\n",
    "                        continue\n",
    "\n",
    "                    elif score in ['dif_threshold', 'dif2_threshold']:\n",
    "                        for score_thresh in scores_thresholds:\n",
    "                            acc = run_experiment(synsets=val_synsets, y=y_val, allowed=list(allow), score=score, score_thresh=score_thresh, merge=merge, thresh=thresh, hide_warnings=True)\n",
    "                            results_allow[allow][(score, score_thresh, merge, thresh)] = acc\n",
    "                    else:\n",
    "                        acc = run_experiment(synsets=val_synsets, y=y_val, allowed=list(allow), score=score, merge=merge, thresh=thresh, hide_warnings=True)\n",
    "                        results_allow[allow][(score, None, merge, thresh)] = acc\n",
    "\n",
    "    best_acc = max([max(results_allow[allow].values()) for allow in results_allow.keys()])\n",
    "    best_params = [(allow, params) for allow in results_allow.keys() for params, acc in results_allow[allow].items() if acc == best_acc]\n",
    "\n",
    "    return best_params, best_acc, results_allow\n",
    "\n",
    "\n",
    "def run_experiment(synsets:list, y:list, allowed:list, score:str, merge:str, thresh:float, score_thresh:float = 0, hide_warnings:bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Run an experiment with a set of parameters.\n",
    "\n",
    "    Parameters:\n",
    "        synsets (list): List of synsets.\n",
    "        y (list): List of labels.\n",
    "        allowed (list): Allowed POS tags.\n",
    "        score (str): Score to compute.\n",
    "        merge (str): Method to merge scores.\n",
    "        thresh (float): Threshold for discretization.\n",
    "        score_thresh (float): Threshold for 'dif_threshold' and 'dif2_threshold' scores.\n",
    "        hide_warnings (bool): Whether to hide warnings or not.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy obtained with the given parameters.\n",
    "    \"\"\"\n",
    "    scores_opinions = []\n",
    "    for opinion in synsets:\n",
    "        scores_sentences = []\n",
    "        for sentence in opinion:\n",
    "            filter_sentence = [name for name in sentence if name.split('.')[1] in allowed]\n",
    "            scores_sentences.append(score_synsets(synsets=filter_sentence, score=score, merge_scores=merge, score_threshold=score_thresh, hide_warnings=hide_warnings))\n",
    "\n",
    "        scores_opinions.append(np.mean(scores_sentences))\n",
    "\n",
    "    results_opinions = discretize_scores(scores=scores_opinions, threshold=thresh)\n",
    "\n",
    "    accuracy = accuracy_score(y, results_opinions)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation: 100.00%\r"
     ]
    }
   ],
   "source": [
    "best_params_val, best_acc_val, results_val = validation(val_synsets=val_synsets, y_val=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for validation (Accuracy: 0.6507):\n",
      "\t*ALLOWED POS TAGS: ('n', 'a', 'v'); PARAMETERS: Score: dif, Score threshold: None, Merge: sum, Threshold: 0.1\n",
      "\t*ALLOWED POS TAGS: ('n', 'a', 'v'); PARAMETERS: Score: dif_threshold, Score threshold: 0.1, Merge: sum, Threshold: 0.1\n",
      "\t*ALLOWED POS TAGS: ('n', 'a', 's', 'v'); PARAMETERS: Score: dif, Score threshold: None, Merge: sum, Threshold: 0.1\n",
      "\t*ALLOWED POS TAGS: ('n', 'a', 's', 'v'); PARAMETERS: Score: dif_threshold, Score threshold: 0.1, Merge: sum, Threshold: 0.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best parameters for validation (Accuracy: {best_acc_val:.4f}):\")\n",
    "for allow, params in best_params_val:\n",
    "\tprint(f\"\\t*ALLOWED POS TAGS: {allow}; PARAMETERS: Score: {params[0]}, Score threshold: {params[1]}, Merge: {params[2]}, Threshold: {params[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best allowed POS tags for validation: ('n', 'a', 'v')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "('pos', None, 'sum', -0.6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Cai Selvas Sala\\GIA_UPC\\2nC\\2nQ\\PLH\\Practica2\\sentiment-analysis\\unsupervised.ipynb Cell 35\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/2nQ/PLH/Practica2/sentiment-analysis/unsupervised.ipynb#X56sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, score \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(scores):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/2nQ/PLH/Practica2/sentiment-analysis/unsupervised.ipynb#X56sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \t\u001b[39mfor\u001b[39;00m j, merge \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(merges):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/2nQ/PLH/Practica2/sentiment-analysis/unsupervised.ipynb#X56sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \t\tresults_matrix[i, j] \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(results_best_allow_comb_val[(score[\u001b[39m0\u001b[39;49m], score[\u001b[39m1\u001b[39;49m], merge, thresh)], \u001b[39m4\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/2nQ/PLH/Practica2/sentiment-analysis/unsupervised.ipynb#X56sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m sns\u001b[39m.\u001b[39mheatmap(results_matrix, annot\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, xticklabels\u001b[39m=\u001b[39mmerges, yticklabels\u001b[39m=\u001b[39mscores_ticks, fmt\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mg\u001b[39m\u001b[39m'\u001b[39m, cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcoolwarm\u001b[39m\u001b[39m'\u001b[39m, cbar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, cbar_kws\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/2nQ/PLH/Practica2/sentiment-analysis/unsupervised.ipynb#X56sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mMerge method\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: ('pos', None, 'sum', -0.6)"
     ]
    }
   ],
   "source": [
    "# If there is only one combination of allowed POS tags with the best maximum accuracy, we choose that one\n",
    "if np.all([best_params_val[0][0] == x[0] for x in best_params_val]):\n",
    "\tbest_allow_comb_val = best_params_val[0][0]\n",
    "# If there are multiple combinations of allowed POS tags with the best maximum accuracy, \n",
    "# we choose the one with the best average accuracy. \n",
    "# If there are still multiple combinations, we choose the one with the fewest allowed POS tags.\n",
    "else:\n",
    "\twarnings.warn(\"Multiple combinations of allowed POS tags with the same maximum accuracy. Choosing the one with the best average accuracy and fewest allowed POS tags.\", RuntimeWarning)\n",
    "\tbest_allow_comb_val = min(\n",
    "\t\tmax(best_params_val, \n",
    "\t  \tkey=lambda t: np.mean([acc for acc in results_val[t[0]].values()])), \n",
    "\tkey=lambda t: len(t[0]))\n",
    "\n",
    "print(f\"Best allowed POS tags for validation: {best_allow_comb_val}\")\n",
    "\n",
    "results_best_allow_comb_val = results_val[best_allow_comb_val]\n",
    "scores = [('pos', None), ('neg', None), ('obj', None), ('max_score', None), ('dif', None), ('dif2', None)]\n",
    "scores_thresholds = [0.05, 0.1, 0.25, 0.4, 0.6]\n",
    "scores = scores + [('dif_threshold', score_thresh) for score_thresh in scores_thresholds] + [('dif2_threshold', score_thresh) for score_thresh in scores_thresholds]\n",
    "merges = ['sum', 'mean', 'max', 'min', 'scale_norm1_mean', 'scale_norm2_mean']\n",
    "thresholds = [-0.6, -0.4, -0.25, -0.1, 0, 0.1, 0.25, 0.4, 0.6]\n",
    "scores_ticks = [f\"{score[0]} ({score[1]})\" if score[1] is not None else score[0] for score in scores]\n",
    "\n",
    "for thresh in thresholds:\n",
    "\tresults_matrix = np.zeros((len(scores), len(merges)))\n",
    "\tfor i, score in enumerate(scores):\n",
    "\t\tfor j, merge in enumerate(merges):\n",
    "\t\t\tresults_matrix[i, j] = round(results_best_allow_comb_val[(score[0], score[1], merge, thresh)], 4)\n",
    "\n",
    "\tsns.heatmap(results_matrix, annot=True, xticklabels=merges, yticklabels=scores_ticks, fmt='g', cmap='coolwarm', cbar=True, cbar_kws={'label': 'Accuracy'})\n",
    "\tplt.xlabel('Merge method')\n",
    "\tplt.ylabel('Score method')\n",
    "\tplt.title(f'Accuracy heatmap validation (Allowed POS tags: {best_allow_comb_val}, Threshold: {thresh})')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cai Selvas Sala\\AppData\\Local\\Temp\\ipykernel_16016\\683027773.py:80: SyntaxWarning: Empty synsets list. Returning default score (0).\n",
      "  warnings.warn(f\"Empty synsets list. Returning default score ({default}).\", SyntaxWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.538"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single execution with test set\n",
    "run_experiment(synsets=test_synsets, y=y_test, allowed=['n', 'a', 's', 'v'], score='dif', merge='sum', thresh=0.1, score_thresh=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "results = []\n",
    "scores_obj = []\n",
    "scores_res = []\n",
    "for opinion in test_synsets:\n",
    "    total_pos = 0\n",
    "    total_neg = 0\n",
    "    total_obj = 0\n",
    "    for sentence in opinion:\n",
    "        filter_sentence = [name for name in sentence if name.split('.')[1] in allowed]\n",
    "        scores = [get_sentiment(syn) for syn in filter_sentence if get_sentiment(syn) != None]\n",
    "        if len(scores) > 0:\n",
    "            total_pos += sum(s[0] for s in scores if s[0] > 0.5) / len(scores)\n",
    "            total_neg += sum(s[1] for s in scores if s[1] > 0.5) /len(scores)\n",
    "            total_obj += sum(s[2] for s in scores) /len(scores)\n",
    "    score = total_obj\n",
    "    scores_obj.append(total_obj)\n",
    "    scores_res.append(total_pos - total_neg)\n",
    "    if score > 0.15:\n",
    "        # print(\"Positive\")\n",
    "        results.append(1)\n",
    "    elif score < 0.15:\n",
    "        # print(\"Negative\")\n",
    "        results.append(0)\n",
    "    else:\n",
    "        # print(\"Neutral\")\n",
    "        results.append(0)\n",
    "results = [0 if a < 0  else 1 for a in scores_res]\n",
    "print(accuracy_score(y_test, results))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other approaches that can be also useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "scores_opinions = []\n",
    "for opinion in X_test:\n",
    "    scores_sentences = []\n",
    "    for sentence in sent_tokenize(opinion):\n",
    "        scores = analyzer.polarity_scores(sentence)\n",
    "        resta = scores[\"pos\"] - scores[\"neg\"]\n",
    "        scores_sentences.append(resta)\n",
    "    scores_opinions.append(np.mean(scores_sentences))\n",
    "\n",
    "results_opinions = discretize_scores(scores=scores_opinions, threshold=0.06)\n",
    "\n",
    "accuracy_score(y_test, results_opinions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.668"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_opinions = discretize_scores(scores=scores_opinions, threshold=0.015)\n",
    "accuracy_score(y_test, results_opinions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
