{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SENTIMENT ANALYSIS - UNSUPERVISED**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Optional\n",
    "import warnings\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets    \n",
    "with open('./data/original_data/X_test.json', 'r') as file:\n",
    "    X_test = json.load(file)\n",
    "    \n",
    "with open('./data/original_data/y_test.json', 'r') as file:\n",
    "    y_test = json.load(file)\n",
    "\n",
    "with open('./data/original_data/X_val.json', 'r') as file:\n",
    "    X_val = json.load(file)\n",
    "\n",
    "with open('./data/original_data/y_val.json', 'r') as file:\n",
    "    y_val = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_text(text:str) -> str:\n",
    "    \"\"\"\n",
    "    Fix text by removing unwanted characters and adding spaces where needed.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): Text to fix.\n",
    "\n",
    "    Returns:\n",
    "        str: Fixed text.\n",
    "    \"\"\"\n",
    "    fixed_text = re.sub(r'[\\\\\"\\+\\/]', '', text)\n",
    "    fixed_text = re.sub(r'\\s*([.,!\\?;:])\\s*', r'\\1 ', fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' s\\s+', \"'s \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' t\\s+', \"'t \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' re\\s+', \"'re \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' ve\\s+', \"'ve \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+\\' ll\\s+', \"'ll \", fixed_text)\n",
    "    fixed_text = re.sub(r'\\s+', ' ', fixed_text)\n",
    "    fixed_text = re.sub(r'\\si\\s', ' I ', fixed_text)\n",
    "    fixed_text = re.sub(r'(?:^|(?<=[.!?]))\\s*(\\w)', lambda x: x.group(1).upper(), fixed_text)\n",
    "    return fixed_text.strip()\n",
    "\n",
    "def get_sentences(text:str, remove_stopwords:bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Get sentences from a text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): Text to analyze.\n",
    "        remove_stopwords (bool): Whether to remove stopwords or not.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of sentences.\n",
    "    \"\"\"\n",
    "    sent_list = nltk.sent_tokenize(text)\n",
    "    if remove_stopwords:\n",
    "        no_stopwords_sentences = []\n",
    "        for sentence in sent_list:\n",
    "            word_tokens = word_tokenize(sentence)\n",
    "            filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "            filtered_sentence = []\n",
    "            for w in word_tokens:\n",
    "                if w not in stop_words:\n",
    "                    filtered_sentence.append(w)\n",
    "            no_stopwords_sentences.append(filtered_sentence)\n",
    "        return no_stopwords_sentences\n",
    "    else:\n",
    "        return sent_list\n",
    "    \n",
    "\n",
    "def get_sentiment(synset:'Synset') -> Optional[tuple]:\n",
    "    \"\"\"\n",
    "    Get sentiment scores for a synset.\n",
    "\n",
    "    Parameters:\n",
    "        synset (Synset): Synset to analyze.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple with positive, negative and objective scores if synset is found. None otherwise.\n",
    "    \"\"\"\n",
    "    sentiment = swn.senti_synset(synset)\n",
    "    return (sentiment.pos_score(), sentiment.neg_score(), sentiment.obj_score()) if sentiment else None\n",
    "\n",
    "def score_synsets(synsets:list, score:str = 'obj', score_threshold:float = 0, merge_scores:str = 'mean', default:float = 0, hide_warnings:bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Compute a score for each synset in a list of synsets and merge them into a single score.\n",
    "\n",
    "    Parameters:\n",
    "        synsets (list): List of synsets.\n",
    "        score (str): Score to compute. One of 'pos', 'neg', 'obj', 'max_score', 'dif', 'dif2', 'dif_threshold', 'dif2_threshold'.\n",
    "        score_threshold (float): Threshold for 'dif_threshold' and 'dif2_threshold' scores. Scores below this threshold are set to 0.\n",
    "        merge_scores (str): Method for merging scores into a single score. One of 'sum', 'mean', 'max', 'min', 'scale_norm1_mean', 'scale_norm2_mean'.\n",
    "        default (float): Default score to return if synsets is empty.\n",
    "        hide_warnings (bool): Whether to hide warnings or not.\n",
    "        \n",
    "    Returns:\n",
    "        float: Merged score.\n",
    "    \"\"\"\n",
    "    if len(synsets) == 0:\n",
    "        if not hide_warnings:\n",
    "            warnings.warn(f\"Empty synsets list. Returning default score ({default}).\", SyntaxWarning)\n",
    "        return default\n",
    "\n",
    "    if score == 'max_score' and merge_scores not in ['sum', 'mean']:\n",
    "        if not hide_warnings:\n",
    "            warnings.warn(f\"Score 'max_score' is not compatible with '{merge_scores}'. Using 'sum' instead.\", SyntaxWarning)\n",
    "        merge_scores = 'sum'\n",
    "\n",
    "    dict_score = {\n",
    "        'pos': lambda s: s[0],\n",
    "        'neg': lambda s: s[1], \n",
    "        'obj': lambda s: s[2],\n",
    "        'max_score': lambda s: (-1 if s[0] > s[1] else 1) if s[0] != s[1] else 0,\n",
    "        'dif': lambda s: s[0] - s[1],\n",
    "        'dif2': lambda s: s[0]**2 - s[1]**2,\n",
    "        'dif_threshold': lambda s: (s[0] if abs(s[0]) >= score_threshold else 0) - (s[1] if abs(s[1]) >= score_threshold else 0),\n",
    "        'dif2_threshold': lambda s: (s[0]**2 if abs(s[0]) >= score_threshold else 0) - (s[1]**2 if abs(s[1]) >= score_threshold else 0),\n",
    "        }\n",
    "    \n",
    "    assert score in dict_score.keys(), f\"Score '{score}' not valid. Choose one of {list(dict_score.keys())}\"\n",
    "    \n",
    "    def min_max_scale(scores:list[float|int]) -> list[float]:\n",
    "        \"\"\"\n",
    "        Compute min-max scaling of a list of scores.\n",
    "\n",
    "        Parameters:\n",
    "            scores (list): List of scores.\n",
    "\n",
    "        Returns:\n",
    "            list: Scaled scores.\n",
    "        \"\"\"\n",
    "        min_score = min(scores)\n",
    "        max_score = max(scores)\n",
    "        return [(s - min_score) / (max_score - min_score) for s in scores]\n",
    "\n",
    "    dict_merge = {\n",
    "        'sum': lambda sc: sum(sc),\n",
    "        'mean': lambda sc: np.mean(sc),\n",
    "        'max': lambda sc: max(sc),\n",
    "        'min': lambda sc: min(sc),\n",
    "        'scale_norm1_mean': lambda sc: np.mean(np.abs(min_max_scale(sc))),\n",
    "        'scale_norm2_mean': lambda sc: np.linalg.norm(min_max_scale(sc)) / len(sc),\n",
    "    }\n",
    "    \n",
    "    assert merge_scores in dict_merge.keys(), f\"Merge score '{merge_scores}' not valid. Choose one of {list(dict_merge.keys())}\"\n",
    "\n",
    "    score_func = dict_score[score]\n",
    "    scores = [score_func(get_sentiment(synset=s)) for s in synsets if s is not None]\n",
    "\n",
    "    if merge_scores in ['scale_norm1_mean', 'scale_norm2_mean'] and min(scores) == max(scores):\n",
    "        if not hide_warnings:\n",
    "            warnings.warn(f\"Scores are all the same and cannot be scaled. Returning default score ({default}).\", RuntimeWarning)\n",
    "        return default\n",
    "\n",
    "\n",
    "    merge_func = dict_merge[merge_scores]\n",
    "    scores_merged = merge_func(scores)\n",
    "\n",
    "    return scores_merged\n",
    "\n",
    "def discretize_scores(scores:list, threshold:float, positive_value = 1, negative_value = 0) -> list:\n",
    "    \"\"\"\n",
    "    Returns a list of binary values based on a threshold.\n",
    "\n",
    "    Parameters:\n",
    "        scores (list): List of scores.\n",
    "        threshold (float): Minimum value to consider a score as positive.\n",
    "        positive_value: Value to assign to positive scores.\n",
    "        negative_value: Value to assign to negative scores.\n",
    "    \"\"\"\n",
    "    return list(map(lambda x: positive_value if x >= threshold else negative_value, scores))\n",
    "def negativize(sentence):\n",
    "    \"\"\"\n",
    "    Based on https://gist.github.com/UtkarshRedd/3fbfd354ea7a6f83bd8f9419a27b0543\n",
    "    \"\"\"\n",
    "    for i in range(2, len(sentence)):\n",
    "        for distance in [1,2]:\n",
    "            if sentence[i-distance] in ['not', \"n't\", \"t\", \"DELETED\"]:\n",
    "                antonyms = []\n",
    "                for syn in wn.synsets(sentence[i]):\n",
    "                    for l in syn.lemmas():\n",
    "                        if l.antonyms():\n",
    "                            antonyms.append(l.antonyms()[0].name())\n",
    "                max_dissimilarity = 0\n",
    "                antonym_max = None\n",
    "                for ant in antonyms:\n",
    "                    for syn in wn.synsets(ant):\n",
    "                        w1 = wn.synsets(sentence[i])[0].name()\n",
    "                        w2 = syn.name()\n",
    "                        word1 = wn.synset(w1)\n",
    "                        word2 = wn.synset(w2)\n",
    "                        if isinstance(word1.path_similarity(word2), (float, int)):\n",
    "                            temp = 1 - word1.path_similarity(word2)\n",
    "                            if temp > max_dissimilarity:\n",
    "                                max_dissimilarity = temp\n",
    "                                antonym_max = ant\n",
    "                if antonym_max:\n",
    "                    sentence[i] = antonym_max\n",
    "                    sentence[i-distance] = 'DELETED'\n",
    "    sentence = [word for word in sentence if word != 'DELETED']\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LESK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lesk_synsets(text:str, lemmatize:bool = True, remove_stopwords:bool = False):\n",
    "    tokens = word_tokenize(text)\n",
    "    if lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    if remove_stopwords:\n",
    "        tokens = [w for w in tokens if not w.lower() in stop_words]\n",
    "    tagged_tokens = [(a.text, a.pos_) for a in nlp(text)]\n",
    "    words = []\n",
    "    for token, pos in tagged_tokens:\n",
    "        if pos == \"NOUN\":\n",
    "            syn = lesk(tokens, token, pos=\"n\")\n",
    "        elif pos == \"ADJ\":\n",
    "            syn = lesk(tokens, token) # pos \"a\" if not want s\n",
    "        elif pos == \"ADV\":\n",
    "            syn = lesk(tokens, token, pos=\"r\")\n",
    "        elif pos == \"VERB\":\n",
    "            syn = lesk(tokens, token, pos=\"v\")\n",
    "        else:\n",
    "            syn = None\n",
    "        if syn is not None:\n",
    "            words.append(syn)\n",
    "    return words\n",
    "\n",
    "def get_lesk_all_synsets(sentences:list, lemmatizer:bool = True) -> list:\n",
    "    all = []\n",
    "    for sentence in sentences:\n",
    "        all.append(get_lesk_synsets(sentence, lemmatizer))\n",
    "    return all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_synsets = []\n",
    "for opinion in X_test:\n",
    "    s = get_sentences(opinion)\n",
    "    #s = [fix_text(t) for t in s]\n",
    "    syns = get_lesk_all_synsets(s, lemmatizer=False)\n",
    "    names = [[syn.name() for syn in ll] for ll in syns]\n",
    "    test_synsets.append(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/lesk_test_synsets.json', 'w') as file:\n",
    "    json.dump(test_synsets, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_synsets = []\n",
    "for opinion in X_val:\n",
    "    s = get_sentences(opinion)\n",
    "    #s = [fix_text(t) for t in s]\n",
    "    syns = get_lesk_all_synsets(s, lemmatizer=False)\n",
    "    names = [[syn.name() for syn in ll] for ll in syns]\n",
    "    val_synsets.append(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/synsets/lesk_val_synsets.json', 'w') as file:\n",
    "\tjson.dump(val_synsets, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UKB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Own implementation of UKB since TextServer didn't allow us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ukb import *\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "try:\n",
    "    ukb_graph = load_ukb_graph(\"ukb_graph.gexf\")\n",
    "except:\n",
    "    print(\"Creating graph...\")\n",
    "    ukb_graph = build_ukb_graph()\n",
    "    nx.write_gexf(ukb_graph, \"ukb_graph.gexf\")\n",
    "\n",
    "ukb = UKB(ukb_graph)\n",
    "def get_ukb_synsets(text:str):\n",
    "    context_words = extract_context_words(text)\n",
    "    disambiguated_senses = ukb.disambiguate_context(context_words, method=3)\n",
    "    return list(disambiguated_senses.values())\n",
    "\n",
    "def get_ukb_all_synsets(sentences:list) -> list:\n",
    "    all = []\n",
    "    for sentence in sentences:\n",
    "        all.append([a for a in get_ukb_synsets(sentence) if a != None])\n",
    "    return all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_words = extract_context_words(\"find the solution to this problem\")\n",
    "disambiguated_senses = ukb.disambiguate_context(context_words, method=3)\n",
    "print(disambiguated_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_synsets = []\n",
    "for i, opinion in enumerate(X_test):\n",
    "    s = get_sentences(opinion)\n",
    "    #s = [fix_text(t) for t in s]\n",
    "    syns = get_ukb_all_synsets(s)\n",
    "    test_synsets.append(syns)\n",
    "    print(i, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ukb2_test_synsets.json', 'w') as file:\n",
    "    json.dump(test_synsets, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_synsets = []\n",
    "for i, opinion in enumerate(X_val):\n",
    "    s = get_sentences(opinion)\n",
    "    #s = [fix_text(t) for t in s]\n",
    "    syns = get_ukb_all_synsets(s)\n",
    "    val_synsets.append(syns)\n",
    "    print(i, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/synsets/ukb2_val_synsets.json', 'w') as file:\n",
    "\tjson.dump(val_synsets, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using just the most freqüent synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = json.load(open(\"./data/synsets/word_sense_frequencies_semcor.json\"))\n",
    "\n",
    "def get_freq_synsets(text:str):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = [(a.text, a.pos_) for a in nlp(text)]\n",
    "    words = []\n",
    "    for token, pos in tagged_tokens:\n",
    "        if token not in frequencies.keys():\n",
    "            syn = None\n",
    "        else:\n",
    "            if pos == \"NOUN\":\n",
    "                syn = max(frequencies[token], key=lambda key: frequencies[token][key])\n",
    "            elif pos == \"ADJ\":\n",
    "                syn = max(frequencies[token], key=lambda key: frequencies[token][key])\n",
    "            elif pos == \"ADV\":\n",
    "                syn = max(frequencies[token], key=lambda key: frequencies[token][key])\n",
    "            elif pos == \"VERB\":\n",
    "                syn = max(frequencies[token], key=lambda key: frequencies[token][key])\n",
    "            else:\n",
    "                syn = None\n",
    "        if syn is not None:\n",
    "            words.append(syn.name() if syn.__class__.__name__ == \"Lemma\" else syn)\n",
    "    return words\n",
    "\n",
    "def get_freq_all_synsets(sentences:list) -> list:\n",
    "    all = []\n",
    "    for sentence in sentences:\n",
    "        all.append([a for a in get_freq_synsets(sentence) if a != None])\n",
    "    return all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_synsets = []\n",
    "for i, opinion in enumerate(X_test):\n",
    "    s = get_sentences(opinion)\n",
    "    #s = [fix_text(t) for t in s]\n",
    "    syns = get_freq_all_synsets(s)\n",
    "    test_synsets.append(syns)\n",
    "    print(i, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/synsets/freq_test_synsets.json', 'w') as file:\n",
    "    json.dump(test_synsets, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/synsets/ukb1_test_synsets.json', 'r') as file:\n",
    "\ttest_synsets = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed = [  \"v\", \"a\", \"s\", \"n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_synsets = []\n",
    "\n",
    "for opinion in test_synsets:\n",
    "    new_opinion = []\n",
    "    for sentence in opinion:\n",
    "        filter_sentence = [name for name in sentence if name != \"NE\" and name.split('.')[1] in allowed]\n",
    "        new_filter_sentence = []\n",
    "        for a in filter_sentence:\n",
    "            if \"Lemma('\" in a :\n",
    "                n = a.replace(\"Lemma('\", \"\").replace(\"')\", \"\")\n",
    "                n = n.split(\".\")\n",
    "                n.pop(-1)\n",
    "                n = \".\".join(n)\n",
    "            else:\n",
    "                n = a\n",
    "            try:\n",
    "                get_sentiment(n)\n",
    "                new_filter_sentence.append(n)\n",
    "            except:\n",
    "                print(n)\n",
    "                pass\n",
    "            \n",
    "        filter_sentence = new_filter_sentence\n",
    "        new_opinion.append(filter_sentence)\n",
    "    new_test_synsets.append(new_opinion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/synsets/freq_test_synsets.json', 'w') as file:\n",
    "    json.dump(new_test_synsets, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/synsets/lesk_test_s_synsets.json', 'r') as file:\n",
    "    test_synsets = json.load(file)\n",
    "\n",
    "with open('./data/synsets/lesk_val_s_synsets.json', 'r') as file:\n",
    "    val_synsets = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(val_synsets:list, \n",
    "                          y_val:list, \n",
    "                          allowed:list = [\"n\", \"a\", \"s\", \"v\", \"r\"],\n",
    "                          scores:list = ['pos', 'neg', 'obj', 'max_score', 'dif', 'dif2', 'dif_threshold', 'dif2_threshold'],\n",
    "                          scores_thresholds:list = [0.05, 0.1, 0.25, 0.4, 0.6],\n",
    "                          merges:list = ['sum', 'mean', 'max', 'min', 'scale_norm1_mean', 'scale_norm2_mean'],\n",
    "                          thresholds = [-0.6, -0.4, -0.25, -0.1, 0, 0.1, 0.25, 0.4, 0.6]\n",
    "                          ) -> tuple[list[tuple[tuple, tuple]], float, dict]:\n",
    "    \"\"\"\n",
    "    Run a validation experiment with different parameters to find the best combination.\n",
    "\n",
    "    Parameters:\n",
    "        val_synsets (list): List of validation synsets.\n",
    "        y_val (list): List of validation labels.\n",
    "        allowed (list): Allowed POS tags.\n",
    "        scores (list): List of scores to compute.\n",
    "        scores_thresholds (list): List of thresholds for 'dif_threshold' and 'dif2_threshold' scores.\n",
    "        merges (list): List of methods to merge scores.\n",
    "        thresholds (list): List of thresholds for discretization.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Best parameters (list of all combinations with the best accuracy), best accuracy (float) and results (dict of all combinations with their accuracy).\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    allow_combinations = list(itertools.chain.from_iterable(itertools.combinations(allowed, r) for r in range(1, len(allowed)+1)))\n",
    "    results_allow = {tuple(a): dict() for a in allow_combinations} # Dict of allowed combinations. Each allowed combination has a dict of results\n",
    "    iter = 1\n",
    "    total_iters = len(scores) * len(merges) * len(thresholds) * len(allow_combinations)\n",
    "    for score in scores:\n",
    "        for merge in merges:\n",
    "            for thresh in thresholds:\n",
    "                for allow in results_allow.keys():\n",
    "                    print(f\"Running validation: {(iter/total_iters)*100:.2f}%\", end=\"\\r\")\n",
    "                    iter += 1\n",
    "\n",
    "                    if score == 'max_score' and merge not in ['sum', 'mean']:\n",
    "                        results_allow[tuple(allow)][(score, None, merge, thresh)] = float('nan')\n",
    "                        continue\n",
    "\n",
    "                    elif score in ['dif_threshold', 'dif2_threshold']:\n",
    "                        for score_thresh in scores_thresholds:\n",
    "                            acc = run_experiment(synsets=val_synsets, y=y_val, allowed=list(allow), score=score, score_thresh=score_thresh, merge=merge, thresh=thresh, hide_warnings=True)\n",
    "                            results_allow[allow][(score, score_thresh, merge, thresh)] = acc\n",
    "                    else:\n",
    "                        acc = run_experiment(synsets=val_synsets, y=y_val, allowed=list(allow), score=score, merge=merge, thresh=thresh, hide_warnings=True)\n",
    "                        results_allow[allow][(score, None, merge, thresh)] = acc\n",
    "\n",
    "    best_acc = max([max(results_allow[allow].values()) for allow in results_allow.keys()])\n",
    "    best_params = [(allow, params) for allow in results_allow.keys() for params, acc in results_allow[allow].items() if acc == best_acc]\n",
    "\n",
    "    return best_params, best_acc, results_allow\n",
    "\n",
    "\n",
    "def run_experiment(synsets:list, y:list, allowed:list, score:str, merge:str, thresh:float, score_thresh:float = 0, hide_warnings:bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Run an experiment with a set of parameters.\n",
    "\n",
    "    Parameters:\n",
    "        synsets (list): List of synsets.\n",
    "        y (list): List of labels.\n",
    "        allowed (list): Allowed POS tags.\n",
    "        score (str): Score to compute.\n",
    "        merge (str): Method to merge scores.\n",
    "        thresh (float): Threshold for discretization.\n",
    "        score_thresh (float): Threshold for 'dif_threshold' and 'dif2_threshold' scores.\n",
    "        hide_warnings (bool): Whether to hide warnings or not.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy obtained with the given parameters.\n",
    "    \"\"\"\n",
    "    scores_opinions = []\n",
    "    for opinion in synsets:\n",
    "        scores_sentences = []\n",
    "        for sentence in opinion:\n",
    "            filter_sentence = [name for name in sentence if name.split('.')[1] in allowed]\n",
    "            scores_sentences.append(score_synsets(synsets=filter_sentence, score=score, merge_scores=merge, score_threshold=score_thresh, hide_warnings=hide_warnings))\n",
    "\n",
    "        scores_opinions.append(np.mean(scores_sentences))\n",
    "\n",
    "    results_opinions = discretize_scores(scores=scores_opinions, threshold=thresh)\n",
    "\n",
    "    accuracy = accuracy_score(y, results_opinions)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_val, best_acc_val, results_val = validation(val_synsets=val_synsets, y_val=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best parameters for validation (Accuracy: {best_acc_val:.4f}):\")\n",
    "for allow, params in best_params_val:\n",
    "\tprint(f\"\\t*ALLOWED POS TAGS: {allow}; PARAMETERS: Score: {params[0]}, Score threshold: {params[1]}, Merge: {params[2]}, Threshold: {params[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is only one combination of allowed POS tags with the best maximum accuracy, we choose that one\n",
    "if np.all([best_params_val[0][0] == x[0] for x in best_params_val]):\n",
    "\tbest_allow_comb_val = best_params_val[0][0]\n",
    "# If there are multiple combinations of allowed POS tags with the best maximum accuracy, \n",
    "# we choose the one with the best average accuracy. \n",
    "# If there are still multiple combinations, we choose the one with the fewest allowed POS tags.\n",
    "else:\n",
    "\twarnings.warn(\"Multiple combinations of allowed POS tags with the same maximum accuracy. Choosing the one with the best average accuracy and fewest allowed POS tags.\", RuntimeWarning)\n",
    "\tbest_allow_comb_val = min(\n",
    "\t\tmax(best_params_val, \n",
    "\t  \tkey=lambda t: np.mean([acc for acc in results_val[t[0]].values()])), \n",
    "\tkey=lambda t: len(t[0]))\n",
    "\n",
    "print(f\"Best allowed POS tags for validation: {best_allow_comb_val}\")\n",
    "\n",
    "results_best_allow_comb_val = results_val[best_allow_comb_val]\n",
    "scores = [('pos', None), ('neg', None), ('obj', None), ('max_score', None), ('dif', None), ('dif2', None)]\n",
    "scores_thresholds = [0.05, 0.1, 0.25, 0.4, 0.6]\n",
    "scores = scores + [('dif_threshold', score_thresh) for score_thresh in scores_thresholds] + [('dif2_threshold', score_thresh) for score_thresh in scores_thresholds]\n",
    "merges = ['sum', 'mean', 'max', 'min', 'scale_norm1_mean', 'scale_norm2_mean']\n",
    "thresholds = [-0.6, -0.4, -0.25, -0.1, 0, 0.1, 0.25, 0.4, 0.6]\n",
    "scores_ticks = [f\"{score[0]} ({score[1]})\" if score[1] is not None else score[0] for score in scores]\n",
    "\n",
    "for thresh in thresholds:\n",
    "\tresults_matrix = np.zeros((len(scores), len(merges)))\n",
    "\tfor i, score in enumerate(scores):\n",
    "\t\tfor j, merge in enumerate(merges):\n",
    "\t\t\tresults_matrix[i, j] = round(results_best_allow_comb_val[(score[0], score[1], merge, thresh)], 4)\n",
    "\n",
    "\tsns.heatmap(results_matrix, annot=True, xticklabels=merges, yticklabels=scores_ticks, fmt='g', cmap='coolwarm', cbar=True, cbar_kws={'label': 'Accuracy'})\n",
    "\tplt.xlabel('Merge method')\n",
    "\tplt.ylabel('Score method')\n",
    "\tplt.title(f'Accuracy heatmap validation (Allowed POS tags: {best_allow_comb_val}, Threshold: {thresh})')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single execution with test set\n",
    "run_experiment(synsets=test_synsets, y=y_test, allowed=['n', 'a', 's', 'v'], score='dif', merge='sum', thresh=0.1, score_thresh=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "results = []\n",
    "scores_obj = []\n",
    "scores_res = []\n",
    "for opinion in test_synsets:\n",
    "    total_pos = 0\n",
    "    total_neg = 0\n",
    "    total_obj = 0\n",
    "    for sentence in opinion:\n",
    "        filter_sentence = [name for name in sentence if name.split('.')[1] in allowed]\n",
    "        scores = [get_sentiment(syn) for syn in filter_sentence if get_sentiment(syn) != None]\n",
    "        if len(scores) > 0:\n",
    "            total_pos += sum(s[0] for s in scores if s[0] > 0.5) / len(scores)\n",
    "            total_neg += sum(s[1] for s in scores if s[1] > 0.5) /len(scores)\n",
    "            total_obj += sum(s[2] for s in scores) /len(scores)\n",
    "    score = total_obj\n",
    "    scores_obj.append(total_obj)\n",
    "    scores_res.append(total_pos - total_neg)\n",
    "    if score > 0.15:\n",
    "        # print(\"Positive\")\n",
    "        results.append(1)\n",
    "    elif score < 0.15:\n",
    "        # print(\"Negative\")\n",
    "        results.append(0)\n",
    "    else:\n",
    "        # print(\"Neutral\")\n",
    "        results.append(0)\n",
    "results = [0 if a < 0  else 1 for a in scores_res]\n",
    "print(accuracy_score(y_test, results))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other approaches that can be also useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "scores_opinions = []\n",
    "for opinion in X_test:\n",
    "    scores_sentences = []\n",
    "    for sentence in sent_tokenize(opinion):\n",
    "        scores = analyzer.polarity_scores(sentence)\n",
    "        resta = scores[\"pos\"] - scores[\"neg\"]\n",
    "        scores_sentences.append(resta)\n",
    "    scores_opinions.append(np.mean(scores_sentences))\n",
    "\n",
    "results_opinions = discretize_scores(scores=scores_opinions, threshold=0.06)\n",
    "\n",
    "accuracy_score(y_test, results_opinions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_opinions = discretize_scores(scores=scores_opinions, threshold=0.015)\n",
    "accuracy_score(y_test, results_opinions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
